<!--
页面创建:刘芳宇创建
日期:2021年4月
-->
<!DOCTYPE html>
<html lang="en">

<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- Meta, title, CSS, favicons, etc. -->
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="">
<meta name="keywords" content="">
<meta name="author" content="Fangyu Liu">

<title>Fangyu Liu</title>

<!-- Bootstrap core CSS -->
<link href="./css/bootstrap.min.css" rel="stylesheet">
<link href="./css/custom.css" rel="stylesheet">
<link href="./css/syntax.css" rel="stylesheet">
<link rel="icon" type="image/png" href="icon/bachelor.png">
<!--[if lt IE 9]>-->
<!--	<script src="../static/js/ie8-responsive-file-warning.js"></script>-->
<!--<![endif]-->


<!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
<!--[if lt IE 9]>
  <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
  <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->


  <style type="text/css"></style>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-9372397-3', 'auto');
  ga('send', 'pageview');

</script>
</head>

  <body data-twttr-rendered="true">
      <header class="navbar navbar-static-top bs-docs-nav custom_navbar navbar-fixed-top" id="top" role="banner">
    <div class="color_wrapper"></div>
  <div class="container">
    <div class="navbar-header">
      <button class="navbar-toggle collapsed" type="button" data-bs-toggle="collapse" data-bs-target=".bs-navbar-collapse">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav class="collapse navbar-collapse bs-navbar-collapse">
      <ul class="nav navbar-nav navbar-right">
        <li><a href="#about">个人简介</a></li>
		<li><a href="#research">研究方向</a></li>
		<li><a href="#education">教育背景</a></li>
		<li><a href="#work">工作经历</a></li>
		<li><a href="#projects">参与项目</a></li>
        <li><a href="#publications">近期论文</a></li>
		<li><a href="./team/activity.html">参与活动</a></li>
		<li><a href="#activities">学术服务</a></li>
		  <!--
		<li><a href="#talks">邀请报告</a></li>
        <li><a href="#activity">参与活动</a></li>
		<li><a href="#award">所获奖励</a></li>
		  -->
		<li><a href="#award">荣誉奖项</a></li>
		<li><a href="index_english.html">English Version</a></li>
      </ul>
    </nav>
  </div>
</header>
<div class="nav-space"></div>
  <div class="page-titile-wraper aboutme-wraper">
  <div class="container">
    <div class="row">
      <!-- <div class="col-md-3"><img src="assets/fangyu_liu.jpg" style= "margin-top:20px" class="img-responsive img-circle"></div> -->
	  <!-- 图片列：在手机上占满整行（col-12），中等屏幕以上占3列（col-md-3） -->
	  <div class="col-12 col-md-3 text-center">  <!-- 添加 text-center 使图片居中 -->
			<img src="assets/fangyu_liu.jpg" class="img-responsive img-circle" style="margin: 20px auto;"> 
  		    <!-- 限制最大宽度并居中 -->
      </div>
      <div class="col-md-9">
		<br>
		<div style="display: inline;">
			<img src="icon/bachelor.png" width="30px" height="35px" style="float:left;margin-top: 15px;margin-right: 10px;"/>
        	<h3 id="about" style="position: relative;"><b>刘芳宇 [Fangyu Liu]</b></h3>
		</div>
		<div style="display: inline;">
			<img src="icon/hat.png" width="30px" height="30px" style="float:left;margin-top: 10px;margin-right: 10px;"/>
        	<h3 style="position: relative;"><b>工学硕士</b></h3>
		</div>
		<div style="display: inline;">
			<img src="icon/team.png" width="30px" height="30px" style="float:left;margin-right: 10px;"/>
			<h4 style="position: relative;margin-top: 15px;"><a href="./team/activity.html">参与活动</a></h4>
		</div>
		<div style="display: inline;">
			<img src="icon/profession.png" width="30px" height="30px" style="float:left;margin-right: 10px;"/>
			<h4 style="position: relative;margin-top: 15px;"><a href="https://www.siat.ac.cn/" target="_blank">中国科学院深圳先进技术研究院</a>，<a href="https://www.ucas.ac.cn/" target="_blank">中国科学院大学</a>博士研究生（在读）</h4>
		</div>
		<br>
		<div style="display: inline;">
			<img src="icon/office.png" width="30px" height="35px" style="float:left;margin-top: -23px;margin-right: 10px;"/>
			<h4 style="position: relative;margin-top: -13px;">深圳市南山区西丽深圳大学城学苑大道1068号，邮编：518055</h4>
		</div>
		<!--<h4>电话: +86 130, +3 69</h4>-->
		<div style="display: inline;">
			<img src="icon/email.png" width="30px" height="20px" style="float:left;margin-top: 5px;margin-right: 10px;"/>
			<h4 style="position: relative;margin-top: 15px;">fy dot liu1 at siat dot ac dot cn</h4>
		</div>
		<br>
		<div style="display: inline;">
			<img src="icon/click1.png" width="30px" height="30px" style="float:left;margin-top: -23px;margin-right: 10px;"/>
			<h4 style="position: relative;margin-top: -15px;"><a href="./index_english.html">English Version</a>, <a href="https://scholar.google.com/citations?user=dR2OCZ8AAAAJ">Google Scholar</a>, <a href="https://orcid.org/0000-0001-8332-0154">ORCID</a></h4>
		</div>
		<br>
        <!--<div class="pull-right">
          <a href="#">li1989 {at} mail.dlut.edu.cn</a>
          <a href="./assets/_cv.pdf" target="_blank" class="btn btn-primary btn-md" role="button">
            <span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span>Download my CV
          </a>
        </div>-->
      </div>
    </div>
  </div>
</div>

<div class="container">
	<div class="row">
		<div class="col-md-12" role="main">
			<h2 id="biography" class="page-header">个人简介</h2>
			<div class="row">
				<div class="col-md-12">
					<ul class="biography-list" id="grants0">
						<li>刘芳宇于2020年从南华大学软件工程专业获工学学士学位，于2023年从上海大学计算机应用技术专业获工学硕士学位，他目前正在中国科学院大学攻读计算机应用技术专业博士学位。他目前的研究兴趣包括生物医学信号处理、传感器信息融合、可穿戴健康监测设备、医学图像分析和机器学习。</li>
					</ul>
				</div>
			</div>

			<h2 id="research" class="page-header">研究方向</h2>
			<div class="row">
				<div class="col-md-12">
					<ul class="research-list" id="grants1">
						<li>生物医学信号处理</li>
						<li>传感器信息融合</li>
						<li>可穿戴健康监测设备</li>
<!--						<li>可穿戴计算</li>-->
<!--						<li>多传感器数据融合</li>-->
						<li>医学图像分析</li>
						<li>机器学习</li>
					</ul>
				</div>
			</div>
			<!--
			<h2 id="research" class="page-header">研究领域</h2>
			<div class="row">
				<div class="col-md-12">
					<ul class="paper-list" id="grants1">
						<li>大数据分析</li>
						<li>社交网络分析</li>
						<li>生物信息学</li>
						<li>服务计算</li>
						<li>数据库技术</li>
						<li>业务流程管理</li>
						<li>智能信息处理等</li>
					</ul>
				</div>
			</div>
			-->

			<h2 id="education" class="page-header">教育背景</h2>
			<div class="row">
				<div class="col-md-12">
					<ul class="educate-list-3" id="grants2-3">
						<li>2023.09 - &nbsp;&nbsp;&nbsp;现在&nbsp;&nbsp;&nbsp;：工学博士，计算机应用技术，<a href="https://www.siat.ac.cn/" target="_blank">中国科学院深圳先进技术研究院</a>，<a href="https://www.ucas.ac.cn/" target="_blank">中国科学院大学</a></li>
					</ul>
					<ul class="educate-list-2" id="grants2-2">
						<li>2020.09 - 2023.06：工学硕士，计算机应用技术，<a href="https://cs.shu.edu.cn/" target="_blank">计算机工程与科学学院</a>，<a href="https://www.shu.edu.cn/" target="_blank">上海大学</a>（推荐免试研究生）</li>
					</ul>
					<ul class="educate-list-1" id="grants2-1">
						<li>2016.09 - 2020.06：工学学士，软件工程，<a href="https://jsjxy.usc.edu.cn/" target="_blank">计算机学院 / 软件学院</a>，<a href="https://www.usc.edu.cn/" target="_blank">南华大学</a>（卓越工程师教育培养计划）</li>
					</ul>
				</div>
			</div>

			<h2 id="work" class="page-header">工作经历</h2>
			<div class="row">
				<div class="col-md-12">
					<ul class="work-list-2" id="grants3-2">
						<li>2023.09.19 - 2024.05.07：<a href="https://www.bgi.com/" target="_blank">华大基因</a>联合培养学生</li>
					</ul>
					<ul class="work-list-1" id="grants3-1">
						<li>2019.11.15 - 2020.02.15：核工业工程研究设计有限公司实习生</li>
					</ul>
				</div>
			</div>

			<h2 id="projects" class="page-header">参与项目</h2>
			<div class="row">
				<div class="col-md-12">
					<ol class="project-list" id="grants4">
						<li>
							<b>题目:</b> <u>面向个体化运动健康的可穿戴智能感知与计算方法研究</u> [<a href="https://fangyuliu-2023.github.io/index.html#projects">Link</a>]<br/>
							<b>来源:</b> 深圳市协同创新科技计划-国际科技合作项目<br/>
							<!-- <b>编号:</b> GJHZ20220913142808016<br/> -->
							<b>时间:</b> 2023 - 2025<br/>
							<b>职责:</b> 学生参与
						</li>
					</ol>
				</div>
			</div>
			<div class="row">
				<div class="col-md-12">
					<ol class="project-list" id="grants4">
						<li>
							<b>题目:</b> <u>基于张量的阿尔兹海默症多模态数据融合与因果推理及可解释辅助诊断</u> [<a href="https://fangyuliu-2023.github.io/index.html#projects">Link</a>]<br/>
							<b>来源:</b> 国家重点研发计划“政府间国际科技创新合作”重点专项<br/>
							<!-- <b>编号:</b> 2024YFE0102100<br/> -->
							<b>时间:</b> 2024 - 现在<br/>
							<b>职责:</b> 学生参与
						</li>
					</ol>
				</div>
			</div>

			<h2 id="publications" class="page-header">近期论文 [<a href="paper.html">全部</a>]</h2>
            <div class="papers-container">
                <!-- 论文15 -->
                <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract15.jpg" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract15.jpg'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">DBMNet: Dual-Branch Multi-modal Network with Spatial-Temporal Fusion for Objective Physical Fatigue Assessment</div>
                        <div class="paper-authors"><span class="highlight">Fangyu Liu</span>, Hao Wang, Fangmin Sun*, Xiangchen Li, Ye Li</div>
                        <div class="paper-journal"><b>IEEE Transactions on Consumer Electronics</b>, 2025</div>
                        <div class="paper-extra">中科院 2 区, JCR 1 区, IF:10.9</div>
                        <div class="paper-links">
                            <a href="https://doi.org/10.1109/TCE.2025.3618484" target="_blank">[论文]</a>
                        </div>
                        <div class="paper-abstract">
                            <p>精准的客观身体疲劳（OPF）评估对工业安全、运动优化和健康管理至关重要。尽管基于多源融合的可穿戴OPF监测方法正在兴起，但它们面临三大挑战：空间建模不足、多模态融合低效和长程依赖捕捉有限。为此，我们提出DBMNet——一种面向疲劳等级分类的双分支多模态网络，该网络分别从原始12导联心电图和多部位IMU信号中提取时序动态和空间模式。我们新颖的卷积加性自适应交叉优化融合（CAACF）模块实现了异构模态的自适应可解释融合。基于65名受试者在分级跑步机测试中采集的同步ECG-IMU数据及粗细粒度疲劳标注，实验表明DBMNet在多项指标上相比现有最佳方法准确率提升最高达7%。其轻量级架构确保可部署于移动和可穿戴设备，为基于多源生理信号的实时疲劳监测提供了有效框架。</p>
                            <!-- <p>Accurate objective physical fatigue (OPF) assessment is essential for industrial safety, athletic optimization, and health management. While wearable multisource fusion methods are emerging for OPF monitoring, they face three key challenges: inadequate spatial modeling, inefficient multimodal fusion, and limited long-range dependency capture. To address these limitations, we propose DBMNet - a dual-branch multimodal network for fatigue level classification that separately extracts temporal dynamics and spatial patterns from raw 12-lead ECG and multi-site IMU signals. Our novel Convolutional Additive Adaptive Cross-refinement Fusion (CAACF) module enables adaptive and interpretable fusion of heterogeneous modalities. Using synchronized ECG-IMU data from 65 subjects during graded treadmill exercise with coarse and fine-grained fatigue annotations, experiments show DBMNet achieves up to 7% accuracy improvement over state-of-the-art methods across multiple metrics. The lightweight architecture ensures deployability on mobile and wearable devices, providing an effective framework for real-time fatigue monitoring using multi-source physiological signals.</p> -->
                        </div>
                    </div>
                </div>

                <!-- 论文14 -->
                <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract14.jpg" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract14.jpg'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">Analysis of Sensor Location and Time–Frequency Feature Contributions in IMU-Based Gait Identity Recognition</div>
                        <div class="paper-authors"><span class="highlight">Fangyu Liu</span><sup>1</sup>, Hao Wang<sup>1</sup>, Xiang Li, Fangmin Sun* (<span style="background-color: rgb(252, 248, 227);"><sup>1</sup> <i>equal contribution</i></span>)</div>
                        <div class="paper-journal"><b>Electronics</b>, 2025</div>
                        <div class="paper-extra">中科院 4 区, JCR 2 区, IF:2.6</div>
                        <div class="paper-links">
                            <a href="https://doi.org/10.3390/electronics14193905" target="_blank">[论文]</a>
                        </div>
                        <div class="paper-abstract">
                            <p>IMU步态生物识别因其非侵入性而备受关注，但各传感器位置与信号模态的实际贡献尚不明确。本研究系统量化分析了不同IMU位置（小腿、腰部、手腕）和特征域（时域、频域）在身份识别中的作用。通过注意力门控融合网络自适应加权各信号分支，实验表明：小腿IMU贡献最大，腰部和手腕传感器主要提供辅助信息；时域特征主导分类性能，频域特征提供互补鲁棒性。这些发现为设计高效可穿戴身份识别系统提供了重要的传感器与特征选择依据。</p>
                            <!-- <p>IMU-based gait biometrics have gained attention for unobtrusive identity recognition, yet the actual contributions of different sensor positions and signal modalities remain unclear. This study presents a comprehensive quantitative analysis of IMU placements (shank, waist, wrist) and feature domains (time and frequency) for identity recognition. Using an attention-gated fusion network to adaptively weight signal branches, experiments reveal the shank IMU dominates recognition accuracy while waist and wrist sensors provide auxiliary information. Time-domain features contribute most to classification performance, with frequency-domain features offering complementary robustness. These findings provide crucial guidance for sensor and feature selection in designing efficient wearable identity recognition systems.</p> -->
                        </div>
                    </div>
                </div>

                <!-- 论文13 -->
                <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract13.jpg" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract13.jpg'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">A Transfer Learning Method for Cross-Scenario Abnormal Running Posture Detection Based on Wearable Sensors</div>
                        <div class="paper-authors">Xiang Li, Hao Wang, <span class="highlight">Fangyu Liu</span>, Ye Li, Han Zhang*, Fangmin Sun*</div>
                        <div class="paper-journal"><b>IEEE Transactions on Consumer Electronics</b>, 2025</div>
                        <div class="paper-extra">中科院 2 区, JCR 1 区, IF:10.9</div>
                        <div class="paper-links">
                            <a href="https://doi.org/10.1109/TCE.2025.3600827" target="_blank">[论文]</a>
                        </div>
                        <div class="paper-abstract">
                            <p>本文针对穿戴式惯性传感器检测异常跑步姿态时存在的跨场景适应性差和计算复杂度高的问题，提出了一种跨场景迁移学习框架（CSTL）。该框架分为预训练和微调两阶段：预训练阶段采用轻量化归一化Transformer（LNT）模型，基于单一场景数据提升初始识别精度；微调阶段通过贡献引导的参数冻结和动态适配器策略，将预训练模型迁移至五种跑步场景，并结合留一交叉验证评估性能。实验表明，仅需10轮微调即可在跨场景测试中达到87.01%的准确率，显著优于非迁移学习方法，实现了精度与效率的最佳平衡。该方案有效提升了异常跑步姿态检测在真实场景中的适应性和鲁棒性。</p>
                            <!-- <p>This paper proposes a Cross-Scenario Transfer Learning (CSTL) framework to address the poor adaptability and high complexity of deep learning-based abnormal running posture detection using wearable inertial sensors. The framework includes: (1) A LightNorm Transformer (LNT) model pre-trained on single-scenario data for initial feature extraction; (2) A fine-tuning stage with contribution-guided freezing and dynamic adapters to transfer the model to five running scenarios, evaluated via leave-one-out cross-validation. Results show that CSTL achieves 87.01% accuracy with only 10 fine-tuning epochs, outperforming non-transfer methods and optimizing the accuracy-efficiency trade-off. The solution enhances robustness for real-world applications.</p> -->
                        </div>
                    </div>
                </div>

                <!-- 论文12 -->
                <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract12.jpg" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract12.jpg'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">MixMatch-based semi-supervised learning approach for cross-domain locomotion and transportation mode recognition</div>
                        <div class="paper-authors">Hao Wang<sup>1</sup>, <span class="highlight">Fangyu Liu</span><sup>1</sup>, Xiang Li, Huazhen Huang, Ye Li, Fangmin Sun* (<span style="background-color: rgb(252, 248, 227);"><sup>1</sup> <i>equal contribution</i></span>)</div>
                        <div class="paper-journal"><b>Companion of the 2025 on ACM International Joint Conference on Pervasive and Ubiquitous Computing</b>, 2025</div>
                        <div class="paper-extra">UbiComp Companion '25, CCF-A, EI</div>
                        <div class="paper-links">
                            <a href="https://doi.org/10.1145/3714394.3756209" target="_blank">[论文]</a>
							<a href="team/activities/Poster5.pdf" target="_blank">[海报]</a>
                        </div>
                        <div class="paper-abstract">
                            <p>利用可穿戴惯性传感器进行跨领域人类活动识别仍是一项具有挑战性的任务，特别是在靶区领域缺乏标注数据的场景下。为此，我们团队（SIAT-BIT）提出了一种基于MixMatch的半监督学习框架，用于运动与交通方式识别。该方法融合了来自多个公共HAR数据集的标注数据，以及苏塞克斯-华为运动交通识别挑战赛任务2（Kyutech IMU）中的未标注数据。该框架通过整合伪标签生成、数据增强、软标签锐化和跨样本混合技术，有效缓解了领域游走性和标签稀缺问题。实验结果表明，所提方法在任务2中取得了76.8%的准确率和76.5%的F1分数，证实了其在无需依赖靶区领域标签的情况下，对现实世界跨领域活动场景建模的有效性。</p>
                            <!-- <p>Cross-domain human activity recognition using wearable inertial sensors remains a challenging task, especially in scenarios where no labeled data is available in the target domain. To address this, our team (SIAT-BIT) propose a semi-supervised learning framework based on MixMatch for locomotion and transportation mode recognition. Our approach leverages labeled data from multiple public HAR datasets and unlabeled data from the Sussex-Huawei Locomotion-Transportation Recognition Challenge Task 2 (Kyutech IMU) dataset. The framework integrates pseudo-label generation, data augmentation, soft label sharpening, and cross-sample mixing to mitigate domain shift and label scarcity. Experimental results demonstrate that the proposed method achieves competitive performance in Task 2 with an accuracy of 76.8% and F1 score of 76.5%, confirming its effectiveness in modeling real-world cross-domain activity scenarios without relying on target domain labels.</p> -->
                        </div>
                    </div>
                </div>

                <!-- 论文11 -->
                <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract11.jpg" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract11.jpg'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">A Sensor-site Hybrid Algorithm Pipeline for Locomotion and Transportation Mode Recognition</div>
                        <div class="paper-authors"><span class="highlight">Fangyu Liu</span><sup>1</sup>, Hao Wang<sup>1</sup>, Huazhen Huang, Xiang Li, Ye Li, Fangmin Sun* (<span style="background-color: rgb(252, 248, 227);"><sup>1</sup> <i>equal contribution</i></span>)</div>
                        <div class="paper-journal"><b>Companion of the 2025 on ACM International Joint Conference on Pervasive and Ubiquitous Computing</b>, 2025</div>
                        <div class="paper-extra">UbiComp Companion '25, CCF-A, EI</div>
                        <div class="paper-links">
                            <a href="https://doi.org/10.1145/3714394.3756200" target="_blank">[论文]</a>
							<a href="team/activities/Poster4.pdf" target="_blank">[海报]</a>
                        </div>
                        <div class="paper-abstract">
                            <p>人类活动识别技术已广泛应用于移动分析、移动健康和智能感知领域。然而现有HAR算法仍面临传感器模态缺失、设备放置位置差异及模型泛化能力有限等挑战。为解决这些问题，苏塞克斯-华为运动识别挑战赛(SHL)提供了用于算法开发的复杂真实场景数据集。本研究团队(SIAT-BIT)提出基于人工特征工程的分类框架，从加速度计、陀螺仪和磁力计的三轴信号及其振幅等12个通道中提取156个统计特征、时域特征和频域特征。实验结果表明，该方法在多种设备放置场景下均表现出强大的识别性能，在验证数据集上取得71.4%的准确率和71.8%的F1分数，证实了该方法的有效性与高效性。</p>
                            <!-- <p>Human Activity Recognition has been widely applied in mobile analysis, mobile health, and intelligent sensing. However, the existing HAR algorithms still face challenges in sensor modality dropout, varying device placement, and limited model generalization. To address these issues, the Sussex-Huawei Locomotion (SHL) recognition challenge provides a complex real-world dataset for algorithm development. In this study, our team (SIAT-BIT) proposes a classification framework based on handcrafted features, extracting 156 statistical, time-domain, and frequency-domain features from a total of 12 channels, including the tri-axial signals and their amplitudes of accelerometers, gyroscopes, and magnetometers. The experimental results show that our approach achieves strong recognition performance across multiple device placements, achieving an accuracy rate of 71.4% and an F1 score of 71.8% on the verification dataset which confirms the effectiveness and efficiency of our method.</p> -->
                        </div>
                    </div>
                </div>

                <!-- 论文10 -->
                <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract10.jpg" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract10.jpg'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">A Key Feature Screening Method for Human Activity Recognition Based on Multi-head Attention Mechanism</div>
                        <div class="paper-authors">Hao Wang<sup>1</sup>, <span class="highlight">Fangyu Liu</span><sup>1</sup>, Xiang Li, Ye Li, Fangmin Sun* (<span style="background-color: rgb(252, 248, 227);"><sup>1</sup> <i>equal contribution</i></span>)</div>
                        <div class="paper-journal"><b>IEEE International Joint Conference on Biometrics</b>, 2025</div>
                        <div class="paper-extra">IJCB 2025, CCF-C, EI, <i><b>Accepted</b></i></div>
                        <div class="paper-links">
                            <a href="index.html#publications">[论文]</a>
							<a href="team/activities/Poster3.pdf" target="_blank">[海报]</a>
                        </div>
                        <div class="paper-abstract">
                            <p>基于可穿戴传感器的人类活动识别(HAR)在医疗监护、健身监测和智能环境等普适计算应用中至关重要。传感器HAR面临高维多通道时序数据中冗余或无关特征降低性能和可解释性的挑战。我们提出一种多头注意力引导的轻量级特征筛选框架来解决该问题。模型采用通道独立的线性层提取各传感器通道的局部表示，再利用多头注意力模块动态评估跨通道特征重要性。该方法能突出信息性成分同时抑制噪声和冗余。在KU-HAR数据集上，我们的方法仅用60个精选特征就实现了96.0%的准确率。所选特征还为特征选择、模型简化和多模态传感器融合的未来研究提供了重要参考。</p>
                            <!-- <p>Human activity recognition (HAR) using wearable sensors is crucial for ubiquitous computing applications in healthcare, fitness monitoring, and smart environments. Sensor-based HAR faces challenges from high-dimensional, multi-channel time series data containing redundant or irrelevant features that degrade performance and interpretability. We propose a lightweight feature screening framework guided by multi-head attention to address this. The model employs channel-wise linear transformations to extract localized representations from each sensor axis, then uses a multi-head attention module to dynamically assess feature importance across channels. This approach emphasizes informative components while suppressing noise and redundancy. On the KU-HAR dataset, our method achieves 96.0% accuracy using only 60 selected features. The selected features also provide valuable references for future research in feature selection, model simplification and multimodal sensor fusion.</p> -->
                        </div>
                    </div>
                </div>

                <!-- 论文9 -->
                <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract9.png" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract9.png'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">Influencing Factors Mining and Modeling of Energy Expenditure in Running Based on Wearable Sensors</div>
                        <div class="paper-authors"><span class="highlight">Fangyu Liu</span>, Hao Wang, Weilin Zang, Ye Li, Fangmin Sun*</div>
                        <div class="paper-journal"><b>Proceedings of the 2024 International Conference on Sports Technology and Performance Analysis</b>, 2024</div>
                        <div class="paper-extra">ICSTPA 2024, EI</div>
                        <div class="paper-links">
                            <a href="https://doi.org/10.1145/3723936.3723943" target="_blank">[论文]</a>
                            <a href="team/activities/Poster2.pdf" target="_blank">[海报]</a>
                        </div>
                        <div class="paper-abstract">
                            <p>本研究针对基于深度学习的可穿戴能耗监测方法可解释性不足的问题，开发了基于可解释回归算法的跑步能耗实时预测模型。通过从人口统计、身体活动和生理指标三维度分析特征，创新提出手工特征选择方法筛选出743个关键特征。在多种机器学习算法中，梯度提升树（GBR）表现最优（CC=0.970，RMSE=1.004，MAE=0.729）。该研究不仅实现了高精度实时能耗预测，其"手工特征+可解释算法"的建模思路也为运动监测领域提供了新范式。</p>
                            <!-- <p>This study addresses the interpretability limitations of deep learning-based wearable energy expenditure monitoring by developing an explainable regression model for real-time running energy prediction. Through systematic analysis of demographic, physical activity and physiological features, the research proposes a novel hand-crafted feature selection method identifying 743 key features. Among various machine learning algorithms tested, Gradient Boosted Regression (GBR) achieved optimal performance (CC=0.970, RMSE=1.004, MAE=0.729) in five-fold cross-validation with 34 volunteers. The study not only accomplishes accurate real-time energy expenditure prediction but also provides a new technical approach combining manual feature engineering with interpretable algorithms for sports monitoring applications.</p> -->
                        </div>
                    </div>
                </div>
                
                <!-- 论文8 -->
                <!-- <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract8.jpg" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract8.jpg'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">Gait Recognition Method Based on Wearable Sensor Information Fusion</div>
                        <div class="paper-authors">Ruijie Shao, Hao Wang, <span class="highlight">Fangyu Liu</span>, Fangmin Sun*</div>
                        <div class="paper-journal"><b>The 7th International Conference on Biological Information and Biomedical Engineering</b>, 2024</div>
                        <div class="paper-extra">BIBE 2024, EI</div>
                        <div class="paper-links">
                            <a href="https://ieeexplore.ieee.org/abstract/document/10830681" target="_blank">[论文]</a>
                            <a href="team/activities/Poster1.pdf" target="_blank">[海报]</a>
                            <a href="team/activities/BIBE 2024 - Most Influencial Paper Award.pdf" target="_blank">[获奖]</a>
                            <a href="https://mp.weixin.qq.com/s/JQhbjR142JafRyYETr95fg" target="_blank">[宣传报道]</a>
                        </div>
                        <div class="paper-abstract">
                            <p>本研究针对实际步态识别系统中的穿戴设备资源限制和用户体验问题，提出了一种基于多传感器融合的创新方法。通过设计包含注意力机制的轻量化网络模型，在减少参数量的同时提升识别精度。研究系统比较了身体不同部位的传感器性能，确定了最佳个性化配置方案，并通过实验验证数据级融合优于决策级融合。探索了多种身体传感器组合，提出了最优配置方案。通过分析识别规模对模型的影响，证实了该方法的高效数据处理能力和鲁棒性，为实用化步态识别系统开发提供了重要技术支持。</p>
                            <p>This study addresses the challenges of wearable device resource limitations and user experience in practical gait recognition systems by proposing an innovative multi-sensor fusion-based approach. A lightweight network model incorporating an attention mechanism was designed to reduce parameters while improving accuracy. The research systematically compared sensor performance at different body locations to identify optimal individualized configurations, experimentally validating data-level fusion's superiority over decision-level fusion. Various body-worn sensor combinations were investigated to determine the optimal setup. Analysis of subject scale impact confirmed the method's efficient data processing capabilities and robustness, providing important technical support for developing practical gait recognition systems.</p>
                        </div>
                    </div>
                </div> -->
                
                <!-- 论文7 -->
				<!-- <div class="paper-item">
					<div class="paper-image">
						<img src="./graphical abstract/abstract7.png" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract7.png'">
					</div>
					<div class="paper-info">
						<div class="paper-title">Exploring multi-granularity contextual semantics for fully inductive knowledge graph completion</div>
						<div class="paper-authors">Jingchao Wang, Weimin Li*, Alex Munyole Luvembe, Xiao Yu, Xinyi Zhang, <span class="highlight">Fangyu Liu</span>, Fangfang Liu, Hao Wang, Zhenhai Wang, Qun Jin</div>
						<div class="paper-journal"><b>Expert Systems With Applications</b>, 2025</div>
						<div class="paper-extra">CCF-C, 中科院 1 区 TOP, JCR 1 区, IF:7.5</div>
						<div class="paper-links">
							<a href="https://doi.org/10.1016/j.eswa.2024.125407" target="_blank">[论文]</a>
						</div>
						<div class="paper-abstract">
							<p>本研究提出多粒度上下文语义框架MGCS解决全归纳KGC任务，通过路径建模网络（PMN）的创新路径转换策略和子图建模网络（SMN）的高阶语义提取能力，结合对比学习和概念增强编码，显著提升了未见实体/关系的推理性能。实验证明该框架通过比较目标三元组与相似案例的子图语义，实现了更精准的全归纳知识图谱补全。</p>
							<p>This study proposes MGCS, a Multi-Granularity Contextual Semantic framework for fully inductive KGC, addressing limitations in path transformation and high-order semantic utilization. The framework features: (1) a Path Modeling Network (PMN) with innovative conversion strategies to enhance PLMs' path understanding and reliability filtering; (2) a Subgraph Modeling Network (SMN) using interactive GNNs and concept-enhanced encoding to capture high-order semantics via contrastive learning. By comparing subgraph semantics between target triplets and similar cases, MGCS achieves superior performance in handling unseen entities/relations, as validated on benchmark datasets.</p>
						</div>
					</div>
				</div> -->
              
                <!-- 论文6 -->
            	<!-- <div class="paper-item">
					<div class="paper-image">
						<img src="./graphical abstract/abstract6.png" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract6.png'">
					</div>
					<div class="paper-info">
						<div class="paper-title">HAMMF: Hierarchical attention-based multi-task and multi-modal fusion model for computer-aided diagnosis of Alzheimer's disease</div>
						<div class="paper-authors">Xiao Liu, Weimin Li*, Shang Miao, <span class="highlight">Fangyu Liu</span>, Ke Han, Tsigabu Teame Bezabih</div>
						<div class="paper-journal"><b>Computers in Biology and Medicine</b>, 2024</div>
						<div class="paper-extra">中科院 2 区, JCR 1 区, IF:6.3</div>
						<div class="paper-links">
							<a href="https://doi.org/10.1016/j.compbiomed.2024.108564" target="_blank">[论文]</a>
						</div>
						<div class="paper-abstract">
							<p>本研究提出的HAMMF模型通过上下文分层注意力模块（CHAM）从多模态影像中提取特征，结合多任务学习框架，在ADNI 720例数据上实现93.15%的分类准确率。该方法创新性地采用通道-空间双重注意力机制和Transformer特征关联，在保证诊断精度的同时优化模型部署效率，为AD临床诊断提供了高效解决方案。</p>
							<p>This study proposes HAMMF, a Hierarchical Attention-based Multi-task Multi-modal Fusion model for Alzheimer's diagnosis, addressing challenges of model complexity in clinical deployment. The innovative Contextual Hierarchical Attention Module (CHAM) extracts fine-grained features from MRI/PET data via channel-spatial attention mechanisms, while Transformer captures cross-modal correlations. Multi-task learning jointly optimizes AD classification, cognitive score regression and age prediction. Evaluated on 720 ADNI subjects, HAMMF achieves 93.15% AD/NC classification accuracy with demonstrated pathological feature discernment, offering both diagnostic precision and clinical applicability.</p>
						</div>
					</div>
                </div> -->

                <!-- 论文5 -->
                <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract5.png" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract5.png'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">Multi-task joint learning network based on adaptive patch pruning for Alzheimer's disease diagnosis and clinical score prediction</div>
                        <div class="paper-authors"><span class="highlight">Fangyu Liu</span><sup>1</sup>, Shizhong Yuan<sup>1</sup>, Weimin Li*, Qun Xu, Xing Wu, Ke Han*, Jingchao Wang, Shang Miao (<span style="background-color: rgb(252, 248, 227);"><sup>1</sup> <i>equal contribution</i></span>)</div>
                        <div class="paper-journal"><b>Biomedical Signal Processing and Control</b>, 2024</div>
                        <div class="paper-extra">中科院 2 区, JCR 1 区, IF:4.9</div>
                        <div class="paper-links">
                            <a href="https://doi.org/10.1016/j.bspc.2024.106398" target="_blank">[论文]</a>
                        </div>
                        <div class="paper-abstract">
                            <p>本研究针对脑部疾病诊断和临床评分预测中的关键问题，提出了一种创新的多任务联合学习网络（MTJLN）。该方法通过将脑图像划分为216个覆盖所有潜在病变区域的局部图像块，并采用图像块剪枝算法自动筛选信息丰富区域，克服了传统方法需要预先确定判别性位置的局限。创新地在中间层融合基于图像块的细粒度多模态特征和粗粒度非图像特征，充分利用了多模态信息与多任务变量间的内在关联。特别设计的加权损失函数使不完整临床评分的受试者也能参与训练，显著提高了数据利用率。基于ADNI数据库842名受试者的实验验证了该方法在病理分期和临床评分预测方面的有效性，为脑部疾病的精准诊断和进展评估提供了新方案。</p>
                            <!-- <p>This study proposes an innovative Multi-Task Joint Learning Network (MTJLN) to address key challenges in brain disease diagnosis and clinical score prediction. The method divides brain images into 216 local patches covering all potential lesion areas and employs a patch pruning algorithm to automatically select informative regions, overcoming the limitations of pre-determining discriminative locations. The novel framework integrates fine-grained patch-based multimodal features with coarse-grained non-image features at intermediate layers, effectively utilizing intrinsic correlations between multimodal data and multitask variables. A specially designed weighted loss function enables the inclusion of subjects with incomplete clinical scores, significantly improving data utilization. Experiments on 842 ADNI subjects demonstrate the method's effectiveness in predicting pathological stages and clinical scores, providing a new solution for precise brain disease diagnosis and progression assessment.</p> -->
                        </div>
                    </div>
                </div>

                <!-- 论文4 -->
                <!-- <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract4.png" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract4.png'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">ConeE: Global and local context-enhanced embedding for inductive knowledge graph completion</div>
                        <div class="paper-authors">Jingchao Wang, Weimin Li*, Fangfang Liu, Zhenhai Wang, Alex Munyole Luvembe, Qun Jin, Quanke Pan, <span class="highlight">Fangyu Liu</span></div>
                        <div class="paper-journal"><b>Expert Systems With Applications</b>, 2024</div>
                        <div class="paper-extra">CCF-C, 中科院 1 区 TOP, JCR 1 区, IF:7.5</div>
                        <div class="paper-links">
                            <a href="https://doi.org/10.1016/j.eswa.2023.123116" target="_blank">[论文]</a>
                        </div>
                        <div class="paper-abstract">
							<p>本研究提出知识图谱补全（KGC）的新型框架ConeE，通过全局上下文建模模块（GCMM）结合BERT编码器与对比学习提取全局语义，以及局部上下文建模模块（LCMM）采用交互式图神经网络和互信息最大化捕获局部特征，有效解决了归纳式场景下现有方法语义利用不足和稀疏子图性能差的问题。实验验证其显著优于现有最优方法，为知识图谱的归纳推理提供了创新性解决方案。</p>
							<p>This study proposes ConeE, a global and local context-enhanced embedding network for knowledge graph completion (KGC), addressing limitations of existing methods in inductive settings where test entities are unseen during training. The framework features: (1) a Global Context Modeling Module (GCMM) with BERT-based encoder and contrastive learning to extract global semantics, plus a dual-perspective scoring network; (2) a Local Context Modeling Module (LCMM) using interactive GNNs and mutual information maximization for enriched local feature extraction. Experiments demonstrate ConeE's superior performance over state-of-the-art methods, offering an advanced solution for inductive KG reasoning.</p>
                        </div>
                    </div>
                </div> -->

                <!-- 论文3 -->
                <!-- <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract3.png" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract3.png'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">De-accumulated error collaborative learning framework for predicting Alzheimer's disease progression</div>
                        <div class="paper-authors">Hongli Cheng<sup>1</sup>, Shizhong Yuan<sup>1</sup>, Weimin Li*, Xiao Yu, <span class="highlight">Fangyu Liu</span>, Xiao Liu, Tsigabu Teame Bezabih (<span style="background-color: rgb(252, 248, 227);"><sup>1</sup> <i>equal contribution</i></span>)</div>
                        <div class="paper-journal"><b>Biomedical Signal Processing and Control</b>, 2024</div>
                        <div class="paper-extra">中科院 2 区, JCR 1 区, IF:4.9</div>
                        <div class="paper-links">
                            <a href="https://doi.org/10.1016/j.bspc.2023.105767" target="_blank">[论文]</a>
                        </div>
                        <div class="paper-abstract">
							<p>本研究提出LSTM-TSGAIN协同学习框架解决AD进展预测中的缺失数据问题，包含三大创新：1)生成对抗插补降低LSTM误差；2)对抗插补与时间序列模块协同训练；3)可变长度输入适应不同访视次数。基于ADNI 1256例纵向数据的实验验证其优越性，既解决了临床缺失数据难题，也为医学时间序列分析提供了新思路。</p>
							<p>This study proposes an innovative LSTM-TSGAIN collaborative learning framework to address missing data challenges in Alzheimer's disease progression prediction. The framework introduces three key innovations: 1) a generative adversarial imputation method to reduce LSTM prediction errors, 2) collaborative training between adversarial imputation and time-series learning modules, and 3) variable-length inputs accommodating differing subject visit frequencies. Experiments on longitudinal ADNI data from 1256 subjects demonstrate superior performance over state-of-the-art methods, offering both a solution to clinical missing data issues and novel insights for medical time-series analysis.</p>
                        </div>
                    </div>
                </div> -->

                <!-- 论文2 -->
                <!-- <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract2.png" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract2.png'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">MMTFN: Multi-modal Multi-scale transformer fusion network for Alzheimer's disease diagnosis</div>
                        <div class="paper-authors">Shang Miao<sup>#</sup>, Qun Xu<sup>#</sup>, Weimin Li*, Chao Yang*, Bin Sheng, <span class="highlight">Fangyu Liu</span>, Tsigabu T. Bezabih, Xiao Yu (<span style="background-color: rgb(252, 248, 227);"><sup>#</sup> <i>senior authors</i></span>)</div>
                        <div class="paper-journal"><b>International Journal of Imaging Systems and Technology</b>, 2024</div>
                        <div class="paper-extra">中科院 4 区, JCR 2 区, IF:2.5</div>
                        <div class="paper-links">
                            <a href="https://doi.org/10.1002/ima.22970" target="_blank">[论文]</a>
                        </div>
                        <div class="paper-abstract">
							<p>本研究针对阿尔茨海默病（AD）多模态神经影像融合的难题，创新性地提出了多模态多尺度Transformer融合网络（MMTFN）。该网络整合了3D多尺度残差块（3DMRB）和Transformer网络，前者提取多尺度局部病理特征，后者建模长程依赖关系并学习多模态联合表征。基于ADNI数据库720例受试者（含MRI和PET影像）的五项实验验证表明，该方法在AD与正常对照分类中达到94.61%的准确率，显著优于现有方法，为AD的计算机辅助诊断提供了新方案。</p>
							<p>This study proposes a novel Multi-modal Multi-scale Transformer Fusion Network (MMTFN) to address the challenges of fusing multi-modal neuroimaging data for Alzheimer's disease (AD) diagnosis. The network combines 3D Multi-scale Residual Blocks (3DMRB) for extracting local pathological features at different scales with a Transformer network for learning long-range dependencies and joint representations of multi-modal data. Evaluated through five experiments on 720 subjects from ADNI (using MRI and PET images), MMTFN achieved superior performance with 94.61% classification accuracy between AD and Normal Controls, establishing a new effective tool for computer-aided AD diagnosis.</p>
                        </div>
                    </div>
                </div> -->
                
                <!-- 论文1 -->
                <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract1.png" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract1.png'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">Patch-based deep multi-modal learning framework for Alzheimer's disease diagnosis using multi-view neuroimaging</div>
                        <div class="paper-authors"><span class="highlight">Fangyu Liu</span><sup>1</sup>, Shizhong Yuan<sup>1</sup>, Weimin Li*, Qun Xu**, Bin Sheng (<span style="background-color: rgb(252, 248, 227);"><sup>1</sup> <i>equal contribution</i></span>)</div>
                        <div class="paper-journal"><b>Biomedical Signal Processing and Control</b>, 2023</div>
                        <div class="paper-extra">中科院 2 区, JCR 1 区, IF:4.9</div>
                        <div class="paper-links">
                            <a href="https://doi.org/10.1016/j.bspc.2022.104400" target="_blank">[论文]</a>
                        </div>
                        <div class="paper-abstract">
							<p>本研究针对阿尔茨海默病（AD）诊断中单模态方法的局限性以及传统图像块方法的空间信息丢失问题，提出了一种基于图像块的深度多模态学习框架（PDMML）。该框架采用无先验知识的判别性位置发现策略自动筛选病变区域，避免了传统解剖标志检测方法的专家依赖性和病灶漏检；通过在图像块层级融合多模态特征捕获多维度疾病表征，并联合学习局部图像块以保留空间结构信息，解决了图像块展平导致的信息损失问题。基于ADNI数据库842例受试者的实验验证表明，该方法在关键区域定位和疾病诊断方面均显著优于现有方法，为早期轻度认知障碍（MCI）的计算机辅助诊断提供了更优解决方案。</p>
                            <!-- <p>This study addresses the limitations of single-modality methods and spatial information loss in patch-based approaches for Alzheimer's disease (AD) diagnosis by proposing a Patch-based Deep Multi-Modal Learning (PDMML) framework. The framework features a prior-free discriminative location discovery strategy to automatically identify potential lesion areas, eliminating reliance on anatomical landmarks and expert experience. It innovatively integrates multimodal features at the patch level to capture comprehensive disease representations while preserving spatial information through joint patch learning, overcoming the flattening-induced information loss. Evaluated on 842 ADNI subjects, PDMML demonstrates superior performance in both discriminative region localization and brain disease diagnosis, offering a robust computer-aided solution for early mild cognitive impairment (MCI) detection.</p> -->
                        </div>
                    </div>
                </div>
            </div>
			<div class="row">
				<div class="col-md-12">
					<!-- <ol class="papers-list" id="grants5">
						<li>
							<b>Fangyu Liu</b>, Hao Wang, Weilin Zang, Ye Li, Fangmin Sun*. <u> Influencing Factors Mining and Modeling of Energy Expenditure in Running Based on Wearable Sensors</u>. <i><b> International Conference on Sports Technology and Performance Analysis</b>, 2024</i>. (ICSTPA 2024, <i><b>Accept</b></i>)[<a href="https://fangyuliu-2023.github.io/index.html#publications">Link</a>][<a href="team/activities/Poster2.pdf" target="_blank">Poster</a>]
						</li>
						<li>
							Ruijie Shao, Hao Wang, <b>Fangyu Liu</b>, Fangmin Sun*. <u> Gait Recognition Method Based on Wearable Sensor Information Fusion</u>. <i><b> The 7th International Conference on Biological Information and Biomedical Engineering</b>, 2024</i>. (BIBE 2024)[<a href="https://ieeexplore.ieee.org/abstract/document/10830681" target="_blank">Link</a>][<a href="team/activities/Poster1.pdf" target="_blank">Poster</a>][<a href="team/activities/BIBE 2024 - Most Influencial Paper Award.pdf" target="_blank">Award</a>][<a href="https://mp.weixin.qq.com/s/JQhbjR142JafRyYETr95fg" target="_blank">Promotion</a>]
						</li>
						<li>
							Jingchao Wang, Weimin Li*, Alex Munyole Luvembe, Xiao Yu, Xinyi Zhang, <b>Fangyu Liu</b>, Fangfang Liu, Hao Wang, Zhenhai Wang, Qun Jin. <u> Exploring multi-granularity contextual semantics for fully inductive knowledge graph completion</u>. <i><b> Expert Systems With Applications</b>, 2025</i>. (CCF-C, 中科院 1 区 TOP, JCR 1 区, IF:7.5)[<a href="https://doi.org/10.1016/j.eswa.2024.125407" target="_blank">Link</a>]
						</li>
						<li>
							Xiao Liu, Weimin Li*, Shang Miao, <b>Fangyu Liu</b>, Ke Han, Tsigabu Teame Bezabih. <u> HAMMF: Hierarchical attention-based multi-task and multi-modal fusion model for computer-aided diagnosis of Alzheimer's disease</u>. <i><b> Computers in Biology and Medicine</b>, 2024</i>. (中科院 2 区 TOP, JCR 1 区, IF:7)[<a href="https://doi.org/10.1016/j.compbiomed.2024.108564" target="_blank">Link</a>]
						</li>
						<li>
							<b>Fangyu Liu</b>, Shizhong Yuan, Weimin Li*, Qun Xu, Xing Wu, Ke Han*, Jingchao Wang, Shang Miao. <u> Multi-task joint learning network based on adaptive patch pruning for Alzheimer's disease diagnosis and clinical score prediction</u>. <i><b> Biomedical Signal Processing and Control</b>, 2024</i>. (中科院 2 区, JCR 1 区, IF:4.9)[<a href="https://doi.org/10.1016/j.bspc.2024.106398" target="_blank">Link</a>]
						</li>
						<li>
							Jingchao Wang, Weimin Li*, Fangfang Liu, Zhenhai Wang, Alex Munyole Luvembe, Qun Jin, Quanke Pan, <b>Fangyu Liu</b>. <u> ConeE: Global and local context-enhanced embedding for inductive knowledge graph completion</u>. <i><b> Expert Systems With Applications</b>, 2024</i>. (CCF-C, 中科院 1 区 TOP, JCR 1 区, IF:7.5)[<a href="https://doi.org/10.1016/j.eswa.2023.123116" target="_blank">Link</a>]
						</li>
						<li>
							Hongli Cheng, Shizhong Yuan, Weimin Li*, Xiao Yu, <b>Fangyu Liu</b>, Xiao Liu, Tsigabu Teame Bezabih. <u> De-accumulated error collaborative learning framework for predicting Alzheimer's disease progression</u>. <i><b> Biomedical Signal Processing and Control</b>, 2024</i>. (中科院 2 区, JCR 1 区, IF:4.9)[<a href="https://doi.org/10.1016/j.bspc.2023.105767" target="_blank">Link</a>]
						</li>
						<li>
							Shang Miao, Qun Xu, Weimin Li*, Chao Yang*, Bin Sheng, <b>Fangyu Liu</b>, Tsigabu T. Bezabih, Xiao Yu. <u> MMTFN: Multi-modal Multi-scale transformer fusion network for Alzheimer's disease diagnosis</u>. <i><b> International Journal of Imaging Systems and Technology</b>, 2024</i>. (中科院 4 区, JCR 2 区, IF:3)[<a href="https://doi.org/10.1002/ima.22970" target="_blank">Link</a>]
						</li>
						<li>
							<b>Fangyu Liu</b>, Shizhong Yuan, Weimin Li*, Qun Xu**, Bin Sheng. <u> Patch-based deep multi-modal learning framework for Alzheimer's disease diagnosis using multi-view neuroimaging</u>. <i><b> Biomedical Signal Processing and Control</b>, 2023</i>. (中科院 2 区, JCR 1 区, IF:4.9)[<a href="https://doi.org/10.1016/j.bspc.2022.104400" target="_blank">Link</a>]
						</li>
						<li>
							Jingchao Wang, Weimin Li*, Fangfang Liu, Zhenhai Wang, Alex Munyole Luvembe, Qun Jin, Quanke Pan, <b>Fangyu Liu</b>. <u> ConeE: Global and Local Context-enhanced Embedding for Inductive Knowledge Graph Completion</u>. <i><b> Expert Systems With Applications</b>, 2023</i>. (中科院 1 区, JCR 1 区, IF:8.5, <i><b>Accept</b></i>)[<a href="https://fangyuliu-2023.github.io/index.html#publications">Link</a>]
						</li>
					</ol> -->
					<ol class="jump-list" id="grants6">
                        <li>
                            <b><a href="paper.html">更多详情...</a></b>
                        </li>
                    </ol>
				</div>
			</div>

			<h2 id="activities" class="page-header">学术服务</h2>
			<div class="row">
				<div class="col-md-12">
<!--					<ul class="activities-list" id="grants7">-->
<!--						<li>XXX人工智能大会</li>-->
<!--					</ul>-->
<!--					<h4><b>编委会</b></h4>-->
<!--					<ul class="editor-list" id="grants8">-->
<!--						<li>杂志xxxx的Guest Editor</li>-->
<!--						<li>杂志xxxx的Guest Editor</li>-->
<!--						<li>杂志xxxx的Guest Editor</li>-->
<!--					</ul>-->
					<h4><b>审稿人</b></h4>
					<ul class="reviewer-list" id="grants9">
						<li>IEEE Journal of Biomedical and Health Informatics期刊的审稿人</li>
						<li>Knowledge-Based Systems期刊的审稿人</li>
						<li>Scientific Reports期刊的审稿人</li>
						<li>The Journal of Supercomputing期刊的审稿人</li>
						<li>The 21st IEEE International Conference on Ubiquitous Intelligence and Computing (UIC 2024)会议的审稿人</li>
					</ul>
					<h4><b>志愿者</b></h4>
					<ul class="chair-list" id="grants10">
						<li>The 15th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM BCB 2024)会议的志愿者</li>
						<li>“中国科学院第二十届公众科学日”大型公益科普活动志愿者</li>
					</ul>
<!--					<h4><b>程序委员会主席</b></h4>-->
<!--					<ul class="chair-list" id="grants10">-->
<!--						<li>国际会议xxxx的程序主席</li>-->
<!--						<li>国际会议xxxx的程序主席成员</li>-->
<!--						<li>国际会议xxxx的Chair</li>-->
<!--					</ul>-->
<!--					<h4><b>组织者</b></h4>-->
<!--					<ul class="organizer-list" id="grants11">-->
<!--						<li>国际会议xxxx的的组织者</li>-->
<!--					</ul>-->
				</div>
			</div>

			<!--
			<h2 id="teaching" class="page-header">教授课程</h2>
			<div class="row">
				<div class="col-md-12">
					<ul class="teach-list" id="grants12">
						<li>炼丹</li>
						<li>符箓</li>
						<li>法器</li>
					</ul>
				</div>
			</div>
			-->
			<!--
			<h2 id="activity" class="page-header">邀请报告</h2>
			<div class="row">
				<div class="col-md-12">
					<ol class="paper-list" id="grants13">
						<li>
							<b>题目:</b> <u></u> [<a href="./supplement/.pdf" target="_blank">Slide</a>]<br/>
							<b>地点:</b> <br/>
							<b>时间:</b> 2018年10月19日
						</li>
						<li>
							<b>题目:</b> <u></u> [<a href="./supplement/.pdf" target="_blank">Slide</a>]<br/>
							<b>地点:</b> <br/>
							<b>时间:</b> 2018年9月19日
						</li>
					</ol>
				</div>
			</div>
			-->
			<!--
			<h2 id="talks" class="page-header">参与活动</h2>
			<div class="row">
				<div class="col-md-12">
					<h4><b>审稿人</b></h4>
					<ul class="paper-list" id="grants14">
						  <li></li>
					</ul>
					<h4><b>志愿者</b></h4>
					<ul class="paper-list" id="grants15">
						  <li></li>
						  <li></li>
						  <li></li>
					</ul>
					<h4><b>访问</b></h4>
					<ul class="paper-list" id="grants16">
						  <li>2017年10月-12月。应xx大学教授邀请，访问xx大学</a></li>
						  <li>2016年4月-7月。 应华为xxx实验室项目经理xxx邀请访问XX技术有限公司。</li>
					</div>
			</div>
			-->
			<h2 id="award" class="page-header">荣誉奖项</h2>
			<div class="row">
				<div class="col-md-12">
					<ul class="award-list" id="grants17">
						<li>2024-2025, 中国科学院大学年度“三好学生”</li>
<!--						<li>2022-2023, 上海大学年度“二等学业奖学金”</li>-->
<!--						<li>2021-2022, 上海大学年度“二等学业奖学金”</li>-->
<!--						<li>2021-2022, 上海大学计算机工程与科学学院“优秀党支部”</li>-->
<!--						<li>2021-2022, 上海大学研究生党支部“十佳主题党日”</li>-->
<!--						<li>2021, 上海大学研究生暑期社会实践校级优秀项目</li>-->
<!--						<li>2020-2021, 上海大学年度“二等学业奖学金”</li>-->
<!--						<li>2020-2021, 上海大学研究生团支部学生先进集体</li>-->
<!--						<li>2020-2021, 学生社区“积极分子”</li>-->
						<li>2020, 湖南省优秀毕业生</li>
						<li>2020, 南华大学2020届优秀毕业生</li>
<!--						<li>2018-2019, 南华大学年度“一等奖学金”</li>-->
<!--						<li>2018-2019, 南华大学年度“优秀学生干部”</li>-->
<!--						<li>2019, 第十二届中国大学生计算机设计大赛中南地区三等奖</li>-->
<!--						<li>2019, 第十六届五一数学建模竞赛二等奖</li>-->
<!--						<li>2019, 第十届蓝桥杯湖南省C/C++程序设计大学B组二等奖</li>-->
<!--						<li>2018, 第一届“全国高校绿色计算大赛”（开源标注组）全国一等奖</li>-->
						<li>2017-2018, 国家励志奖学金</li>
<!--						<li>2017-2018, 南华大学年度“优秀共青团干部”</li>-->
						<li>2017-2018, 南华大学年度“三好学生”</li>
<!--						<li>2016-2017, 南华大学年度“一等奖学金”</li>-->
<!--						<li>2016-2017, 南华大学年度“优秀学生干部”</li>-->
						<li>2016-2017, 南华大学年度“三好学生”</li>
<!--						<li>2017, 南华大学第十三届ACM程序设计大赛一等奖</li>-->
<!--						<li>2016, 南华大学“学法、守法、用法”小品竞赛一等奖</li>-->
						<li>2015, 高中阶段“三好学生”</li>
					</ul>
				</div>
			</div>
		</div>
	</div>
</div>


<footer class="footer">
	<div class="container">
		<p class="text-muted">Homepage Create:09/2023, Last Modified:10/2025, Copyright © Fangyu Liu 2023-2025</p>
	</div>
	<div class="color_wrapper"></div>
</footer>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
	<script src="./Javascript/jquery.min.js"></script>
    <script src="./Javascript/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 10 bug -->
<!--    <script src="js/ie10-viewport-bug-workaround.js"></script>-->
	<!-- 为按钮添加点击事件 -->
	<script>
		$(document).ready(function(){
		  // 监听窗口宽度变化
		  $(window).resize(function() {
			if ($(window).width() < 768) { // 假设小屏的阈值为768px
			  // 先移除之前的事件监听器，预防尺寸变化重复绑定监听事件
			  $(".navbar-toggle").off('click');
			  $('.navbar-collapse a').off('click');

			  // 监听导航栏按钮的点击事件
			  $(".navbar-toggle").click(function(){
				$(".bs-navbar-collapse").toggleClass("in");
				$(this).toggleClass("collapsed"); // 切换按钮的collapsed类
			  });

			  // 监听导航栏链接的点击事件
			  $('.navbar-collapse a').on('click', function () {
				  $(".navbar-toggle").click(); // 触发导航栏按钮的点击事件，使导航栏收缩
			  });
			}
		  }).trigger('resize'); // 初始触发一次resize事件以应用事件监听器
		});
	</script>
  </body>

</html>