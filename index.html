<!--
页面创建:刘芳宇创建
日期:2021年4月
-->
<!DOCTYPE html>
<html lang="en">

<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- Meta, title, CSS, favicons, etc. -->
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="">
<meta name="keywords" content="">
<meta name="author" content="Fangyu Liu">

<title>Fangyu Liu</title>

<!-- Bootstrap core CSS -->
<link href="./css/bootstrap.min.css" rel="stylesheet">
<link href="./css/custom.css" rel="stylesheet">
<link href="./css/syntax.css" rel="stylesheet">
<link rel="icon" type="image/png" href="icon/bachelor.png">
<!--[if lt IE 9]>-->
<!--	<script src="../static/js/ie8-responsive-file-warning.js"></script>-->
<!--<![endif]-->


<!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
<!--[if lt IE 9]>
  <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
  <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->


  <style type="text/css"></style>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-9372397-3', 'auto');
  ga('send', 'pageview');

</script>
</head>

  <body data-twttr-rendered="true">
      <header class="navbar navbar-static-top bs-docs-nav custom_navbar navbar-fixed-top" id="top" role="banner">
    <div class="color_wrapper"></div>
  <div class="container">
    <div class="navbar-header">
      <button class="navbar-toggle collapsed" type="button" data-bs-toggle="collapse" data-bs-target=".bs-navbar-collapse">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav class="collapse navbar-collapse bs-navbar-collapse">
      <ul class="nav navbar-nav navbar-right">
        <li><a href="#about">个人简介</a></li>
		<li><a href="#research">研究方向</a></li>
		<li><a href="#education">教育背景</a></li>
		<li><a href="#work">工作经历</a></li>
		<li><a href="#projects">参与项目</a></li>
        <li><a href="#publications">近期论文</a></li>
		<li><a href="./team/activity.html">参与活动</a></li>
		<li><a href="#activities">学术服务</a></li>
		  <!--
		<li><a href="#talks">邀请报告</a></li>
        <li><a href="#activity">参与活动</a></li>
		<li><a href="#award">所获奖励</a></li>
		  -->
		<li><a href="#award">荣誉奖项</a></li>
		<li><a href="index_english.html">English Version</a></li>
      </ul>
    </nav>
  </div>
</header>
<div class="nav-space"></div>
  <div class="page-titile-wraper aboutme-wraper">
  <div class="container">
    <div class="row">
      <div class="col-md-3"><img src="assets/fangyu_liu.jpg" style= "margin-top:20px" class="img-responsive img-circle"></div>
      <div class="col-md-9">
		<br>
		<div style="display: inline;">
			<img src="icon/bachelor.png" width="30px" height="35px" style="float:left;margin-top: 15px;margin-right: 10px;"/>
        	<h3 id="about" style="position: relative;"><b>刘芳宇 [Fangyu Liu]</b></h3>
		</div>
		<div style="display: inline;">
			<img src="icon/hat.png" width="30px" height="30px" style="float:left;margin-top: 10px;margin-right: 10px;"/>
        	<h3 style="position: relative;"><b>工学硕士</b></h3>
		</div>
		<div style="display: inline;">
			<img src="icon/team.png" width="30px" height="30px" style="float:left;margin-right: 10px;"/>
			<h4 style="position: relative;margin-top: 15px;"><a href="./team/activity.html">参与活动</a></h4>
		</div>
		<div style="display: inline;">
			<img src="icon/profession.png" width="30px" height="30px" style="float:left;margin-right: 10px;"/>
			<h4 style="position: relative;margin-top: 15px;"><a href="https://www.siat.ac.cn/" target="_blank">中国科学院深圳先进技术研究院</a>，<a href="https://www.ucas.ac.cn/" target="_blank">中国科学院大学</a>博士研究生（在读）</h4>
		</div>
		<br>
		<div style="display: inline;">
			<img src="icon/office.png" width="30px" height="35px" style="float:left;margin-top: -23px;margin-right: 10px;"/>
			<h4 style="position: relative;margin-top: -13px;">深圳市南山区西丽深圳大学城学苑大道1068号，邮编：518055</h4>
		</div>
		<!--<h4>电话: +86 130, +3 69</h4>-->
		<div style="display: inline;">
			<img src="icon/email.png" width="30px" height="20px" style="float:left;margin-top: 5px;margin-right: 10px;"/>
			<h4 style="position: relative;margin-top: 15px;">fy dot liu1 at siat dot ac dot cn</h4>
		</div>
		<br>
		<div style="display: inline;">
			<img src="icon/click1.png" width="30px" height="30px" style="float:left;margin-top: -23px;margin-right: 10px;"/>
			<h4 style="position: relative;margin-top: -15px;"><a href="./index_english.html">English Version</a>, <a href="https://scholar.google.com/citations?user=dR2OCZ8AAAAJ">Google Scholar</a>, <a href="https://orcid.org/0000-0001-8332-0154">ORCID</a></h4>
		</div>
		<br>
        <!--<div class="pull-right">
          <a href="#">li1989 {at} mail.dlut.edu.cn</a>
          <a href="./assets/_cv.pdf" target="_blank" class="btn btn-primary btn-md" role="button">
            <span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span>Download my CV
          </a>
        </div>-->
      </div>
    </div>
  </div>
</div>

<div class="container">
	<div class="row">
		<div class="col-md-12" role="main">
			<h2 id="biography" class="page-header">个人简介</h2>
			<div class="row">
				<div class="col-md-12">
					<ul class="biography-list" id="grants0">
						<li>刘芳宇于2020年从南华大学软件工程专业获工学学士学位，于2023年从上海大学计算机应用技术专业获工学硕士学位，他目前正在中国科学院大学攻读计算机应用技术专业博士学位。他目前的研究兴趣包括生物医学信号处理、传感器信息融合、可穿戴健康监测设备、医学图像分析和机器学习。</li>
					</ul>
				</div>
			</div>

			<h2 id="research" class="page-header">研究方向</h2>
			<div class="row">
				<div class="col-md-12">
					<ul class="research-list" id="grants1">
						<li>生物医学信号处理</li>
						<li>传感器信息融合</li>
						<li>可穿戴健康监测设备</li>
<!--						<li>可穿戴计算</li>-->
<!--						<li>多传感器数据融合</li>-->
						<li>医学图像分析</li>
						<li>机器学习</li>
					</ul>
				</div>
			</div>
			<!--
			<h2 id="research" class="page-header">研究领域</h2>
			<div class="row">
				<div class="col-md-12">
					<ul class="paper-list" id="grants1">
						<li>大数据分析</li>
						<li>社交网络分析</li>
						<li>生物信息学</li>
						<li>服务计算</li>
						<li>数据库技术</li>
						<li>业务流程管理</li>
						<li>智能信息处理等</li>
					</ul>
				</div>
			</div>
			-->

			<h2 id="education" class="page-header">教育背景</h2>
			<div class="row">
				<div class="col-md-12">
					<ul class="educate-list-3" id="grants2-3">
						<li>2023.09 - &nbsp;&nbsp;&nbsp;现在&nbsp;&nbsp;&nbsp;：工学博士，计算机应用技术，<a href="https://www.siat.ac.cn/" target="_blank">中国科学院深圳先进技术研究院</a>，<a href="https://www.ucas.ac.cn/" target="_blank">中国科学院大学</a></li>
					</ul>
					<ul class="educate-list-2" id="grants2-2">
						<li>2020.09 - 2023.06：工学硕士，计算机应用技术，<a href="https://cs.shu.edu.cn/" target="_blank">计算机工程与科学学院</a>，<a href="https://www.shu.edu.cn/" target="_blank">上海大学</a>（推荐免试研究生）</li>
					</ul>
					<ul class="educate-list-1" id="grants2-1">
						<li>2016.09 - 2020.06：工学学士，软件工程，<a href="https://jsjxy.usc.edu.cn/" target="_blank">计算机学院 / 软件学院</a>，<a href="https://www.usc.edu.cn/" target="_blank">南华大学</a>（卓越工程师教育培养计划）</li>
					</ul>
				</div>
			</div>

			<h2 id="work" class="page-header">工作经历</h2>
			<div class="row">
				<div class="col-md-12">
					<ul class="work-list-2" id="grants3-2">
						<li>2023.09.19 - 2024.05.07：<a href="https://www.bgi.com/" target="_blank">华大基因</a>联合培养学生</li>
					</ul>
					<ul class="work-list-1" id="grants3-1">
						<li>2019.11.15 - 2020.02.15：核工业工程研究设计有限公司实习生</li>
					</ul>
				</div>
			</div>

			<h2 id="projects" class="page-header">参与项目</h2>
			<div class="row">
				<div class="col-md-12">
					<ol class="project-list" id="grants4">
						<li>
							<b>题目:</b> <u>面向个体化运动健康的可穿戴智能感知与计算方法研究</u> [<a href="https://fangyuliu-2023.github.io/index.html#projects">Link</a>]<br/>
							<b>来源:</b> 深圳市协同创新科技计划-国际科技合作项目<br/>
							<!-- <b>编号:</b> GJHZ20220913142808016<br/> -->
							<b>时间:</b> 2023 - 2025<br/>
							<b>职责:</b> 学生参与
						</li>
					</ol>
				</div>
			</div>
			<div class="row">
				<div class="col-md-12">
					<ol class="project-list" id="grants4">
						<li>
							<b>题目:</b> <u>基于张量的阿尔兹海默症多模态数据融合与因果推理及可解释辅助诊断</u> [<a href="https://fangyuliu-2023.github.io/index.html#projects">Link</a>]<br/>
							<b>来源:</b> 国家重点研发计划“政府间国际科技创新合作”重点专项<br/>
							<!-- <b>编号:</b> 2024YFE0102100<br/> -->
							<b>时间:</b> 2024 - 现在<br/>
							<b>职责:</b> 学生参与
						</li>
					</ol>
				</div>
			</div>

			<h2 id="publications" class="page-header">近期论文 [<a href="paper.html">全部</a>]</h2>
            <div class="papers-container">
                <!-- 论文9 -->
                <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract9.png" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract9.png'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">Influencing Factors Mining and Modeling of Energy Expenditure in Running Based on Wearable Sensors</div>
                        <div class="paper-authors"><span class="highlight">Fangyu Liu</span>, Hao Wang, Weilin Zang, Ye Li, Fangmin Sun*</div>
                        <div class="paper-journal"><b>Proceedings of the 2024 International Conference on Sports Technology and Performance Analysis</b>, 2024</div>
                        <div class="paper-extra">ICSTPA 2024, EI</div>
                        <div class="paper-links">
                            <a href="https://doi.org/10.1145/3723936.3723943" target="_blank">[论文]</a>
                            <a href="team/activities/Poster2.pdf" target="_blank">[海报]</a>
                        </div>
                        <div class="paper-abstract">
                            <p>本研究针对基于深度学习的可穿戴能耗监测方法可解释性不足的问题，开发了基于可解释回归算法的跑步能耗实时预测模型。通过从人口统计、身体活动和生理指标三维度分析特征，创新提出手工特征选择方法筛选出743个关键特征。在多种机器学习算法中，梯度提升树（GBR）表现最优（CC=0.970，RMSE=1.004，MAE=0.729）。该研究不仅实现了高精度实时能耗预测，其"手工特征+可解释算法"的建模思路也为运动监测领域提供了新范式。</p>
                            <!-- <p>This study addresses the interpretability limitations of deep learning-based wearable energy expenditure monitoring by developing an explainable regression model for real-time running energy prediction. Through systematic analysis of demographic, physical activity and physiological features, the research proposes a novel hand-crafted feature selection method identifying 743 key features. Among various machine learning algorithms tested, Gradient Boosted Regression (GBR) achieved optimal performance (CC=0.970, RMSE=1.004, MAE=0.729) in five-fold cross-validation with 34 volunteers. The study not only accomplishes accurate real-time energy expenditure prediction but also provides a new technical approach combining manual feature engineering with interpretable algorithms for sports monitoring applications.</p> -->
                        </div>
                    </div>
                </div>
                
                <!-- 论文8 -->
                <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract8.jpg" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract8.jpg'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">Gait Recognition Method Based on Wearable Sensor Information Fusion</div>
                        <div class="paper-authors">Ruijie Shao, Hao Wang, <span class="highlight">Fangyu Liu</span>, Fangmin Sun*</div>
                        <div class="paper-journal"><b>The 7th International Conference on Biological Information and Biomedical Engineering</b>, 2024</div>
                        <div class="paper-extra">BIBE 2024, EI</div>
                        <div class="paper-links">
                            <a href="https://ieeexplore.ieee.org/abstract/document/10830681" target="_blank">[论文]</a>
                            <a href="team/activities/Poster1.pdf" target="_blank">[海报]</a>
                            <a href="team/activities/BIBE 2024 - Most Influencial Paper Award.pdf" target="_blank">[获奖]</a>
                            <a href="https://mp.weixin.qq.com/s/JQhbjR142JafRyYETr95fg" target="_blank">[宣传报道]</a>
                        </div>
                        <div class="paper-abstract">
                            <p>本研究针对实际步态识别系统中的穿戴设备资源限制和用户体验问题，提出了一种基于多传感器融合的创新方法。通过设计包含注意力机制的轻量化网络模型，在减少参数量的同时提升识别精度。研究系统比较了身体不同部位的传感器性能，确定了最佳个性化配置方案，并通过实验验证数据级融合优于决策级融合。探索了多种身体传感器组合，提出了最优配置方案。通过分析识别规模对模型的影响，证实了该方法的高效数据处理能力和鲁棒性，为实用化步态识别系统开发提供了重要技术支持。</p>
                            <!-- <p>This study addresses the challenges of wearable device resource limitations and user experience in practical gait recognition systems by proposing an innovative multi-sensor fusion-based approach. A lightweight network model incorporating an attention mechanism was designed to reduce parameters while improving accuracy. The research systematically compared sensor performance at different body locations to identify optimal individualized configurations, experimentally validating data-level fusion's superiority over decision-level fusion. Various body-worn sensor combinations were investigated to determine the optimal setup. Analysis of subject scale impact confirmed the method's efficient data processing capabilities and robustness, providing important technical support for developing practical gait recognition systems.</p> -->
                        </div>
                    </div>
                </div>
                
                <!-- 论文7 -->
				<div class="paper-item">
					<div class="paper-image">
						<img src="./graphical abstract/abstract7.png" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract7.png'">
					</div>
					<div class="paper-info">
						<div class="paper-title">Exploring multi-granularity contextual semantics for fully inductive knowledge graph completion</div>
						<div class="paper-authors">Jingchao Wang, Weimin Li*, Alex Munyole Luvembe, Xiao Yu, Xinyi Zhang, <span class="highlight">Fangyu Liu</span>, Fangfang Liu, Hao Wang, Zhenhai Wang, Qun Jin</div>
						<div class="paper-journal"><b>Expert Systems With Applications</b>, 2025</div>
						<div class="paper-extra">CCF-C, 中科院 1 区 TOP, JCR 1 区, IF:7.5</div>
						<div class="paper-links">
							<a href="https://doi.org/10.1016/j.eswa.2024.125407" target="_blank">[论文]</a>
						</div>
						<div class="paper-abstract">
							<p>本研究提出多粒度上下文语义框架MGCS解决全归纳KGC任务，通过路径建模网络（PMN）的创新路径转换策略和子图建模网络（SMN）的高阶语义提取能力，结合对比学习和概念增强编码，显著提升了未见实体/关系的推理性能。实验证明该框架通过比较目标三元组与相似案例的子图语义，实现了更精准的全归纳知识图谱补全。</p>
							<!-- <p>This study proposes MGCS, a Multi-Granularity Contextual Semantic framework for fully inductive KGC, addressing limitations in path transformation and high-order semantic utilization. The framework features: (1) a Path Modeling Network (PMN) with innovative conversion strategies to enhance PLMs' path understanding and reliability filtering; (2) a Subgraph Modeling Network (SMN) using interactive GNNs and concept-enhanced encoding to capture high-order semantics via contrastive learning. By comparing subgraph semantics between target triplets and similar cases, MGCS achieves superior performance in handling unseen entities/relations, as validated on benchmark datasets.</p> -->
						</div>
					</div>
				</div>
              
                <!-- 论文6 -->
            	<div class="paper-item">
					<div class="paper-image">
						<img src="./graphical abstract/abstract6.png" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract6.png'">
					</div>
					<div class="paper-info">
						<div class="paper-title">HAMMF: Hierarchical attention-based multi-task and multi-modal fusion model for computer-aided diagnosis of Alzheimer's disease</div>
						<div class="paper-authors">Xiao Liu, Weimin Li*, Shang Miao, <span class="highlight">Fangyu Liu</span>, Ke Han, Tsigabu Teame Bezabih</div>
						<div class="paper-journal"><b>Computers in Biology and Medicine</b>, 2024</div>
						<div class="paper-extra">中科院 2 区 TOP, JCR 1 区, IF:7</div>
						<div class="paper-links">
							<a href="https://doi.org/10.1016/j.compbiomed.2024.108564" target="_blank">[论文]</a>
						</div>
						<div class="paper-abstract">
							<p>本研究提出的HAMMF模型通过上下文分层注意力模块（CHAM）从多模态影像中提取特征，结合多任务学习框架，在ADNI 720例数据上实现93.15%的分类准确率。该方法创新性地采用通道-空间双重注意力机制和Transformer特征关联，在保证诊断精度的同时优化模型部署效率，为AD临床诊断提供了高效解决方案。</p>
							<!-- <p>This study proposes HAMMF, a Hierarchical Attention-based Multi-task Multi-modal Fusion model for Alzheimer's diagnosis, addressing challenges of model complexity in clinical deployment. The innovative Contextual Hierarchical Attention Module (CHAM) extracts fine-grained features from MRI/PET data via channel-spatial attention mechanisms, while Transformer captures cross-modal correlations. Multi-task learning jointly optimizes AD classification, cognitive score regression and age prediction. Evaluated on 720 ADNI subjects, HAMMF achieves 93.15% AD/NC classification accuracy with demonstrated pathological feature discernment, offering both diagnostic precision and clinical applicability.</p> -->
						</div>
					</div>
                </div>

                <!-- 论文5 -->
                <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract5.png" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract5.png'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">Multi-task joint learning network based on adaptive patch pruning for Alzheimer's disease diagnosis and clinical score prediction</div>
                        <div class="paper-authors"><span class="highlight">Fangyu Liu</span><sup>1</sup>, Shizhong Yuan<sup>1</sup>, Weimin Li*, Qun Xu, Xing Wu, Ke Han*, Jingchao Wang, Shang Miao (<span style="background-color: rgb(252, 248, 227);"><sup>1</sup> <i>equal contribution</i></span>)</div>
                        <div class="paper-journal"><b>Biomedical Signal Processing and Control</b>, 2024</div>
                        <div class="paper-extra">中科院 2 区, JCR 1 区, IF:4.9</div>
                        <div class="paper-links">
                            <a href="https://doi.org/10.1016/j.bspc.2024.106398" target="_blank">[论文]</a>
                        </div>
                        <div class="paper-abstract">
                            <p>本研究针对脑部疾病诊断和临床评分预测中的关键问题，提出了一种创新的多任务联合学习网络（MTJLN）。该方法通过将脑图像划分为216个覆盖所有潜在病变区域的局部图像块，并采用图像块剪枝算法自动筛选信息丰富区域，克服了传统方法需要预先确定判别性位置的局限。创新地在中间层融合基于图像块的细粒度多模态特征和粗粒度非图像特征，充分利用了多模态信息与多任务变量间的内在关联。特别设计的加权损失函数使不完整临床评分的受试者也能参与训练，显著提高了数据利用率。基于ADNI数据库842名受试者的实验验证了该方法在病理分期和临床评分预测方面的有效性，为脑部疾病的精准诊断和进展评估提供了新方案。</p>
                            <!-- <p>This study proposes an innovative Multi-Task Joint Learning Network (MTJLN) to address key challenges in brain disease diagnosis and clinical score prediction. The method divides brain images into 216 local patches covering all potential lesion areas and employs a patch pruning algorithm to automatically select informative regions, overcoming the limitations of pre-determining discriminative locations. The novel framework integrates fine-grained patch-based multimodal features with coarse-grained non-image features at intermediate layers, effectively utilizing intrinsic correlations between multimodal data and multitask variables. A specially designed weighted loss function enables the inclusion of subjects with incomplete clinical scores, significantly improving data utilization. Experiments on 842 ADNI subjects demonstrate the method's effectiveness in predicting pathological stages and clinical scores, providing a new solution for precise brain disease diagnosis and progression assessment.</p> -->
                        </div>
                    </div>
                </div>

                <!-- 论文4 -->
                <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract4.png" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract4.png'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">ConeE: Global and local context-enhanced embedding for inductive knowledge graph completion</div>
                        <div class="paper-authors">Jingchao Wang, Weimin Li*, Fangfang Liu, Zhenhai Wang, Alex Munyole Luvembe, Qun Jin, Quanke Pan, <span class="highlight">Fangyu Liu</span></div>
                        <div class="paper-journal"><b>Expert Systems With Applications</b>, 2024</div>
                        <div class="paper-extra">CCF-C, 中科院 1 区 TOP, JCR 1 区, IF:7.5</div>
                        <div class="paper-links">
                            <a href="https://doi.org/10.1016/j.eswa.2023.123116" target="_blank">[论文]</a>
                        </div>
                        <div class="paper-abstract">
							<p>本研究提出知识图谱补全（KGC）的新型框架ConeE，通过全局上下文建模模块（GCMM）结合BERT编码器与对比学习提取全局语义，以及局部上下文建模模块（LCMM）采用交互式图神经网络和互信息最大化捕获局部特征，有效解决了归纳式场景下现有方法语义利用不足和稀疏子图性能差的问题。实验验证其显著优于现有最优方法，为知识图谱的归纳推理提供了创新性解决方案。</p>
							<!-- <p>This study proposes ConeE, a global and local context-enhanced embedding network for knowledge graph completion (KGC), addressing limitations of existing methods in inductive settings where test entities are unseen during training. The framework features: (1) a Global Context Modeling Module (GCMM) with BERT-based encoder and contrastive learning to extract global semantics, plus a dual-perspective scoring network; (2) a Local Context Modeling Module (LCMM) using interactive GNNs and mutual information maximization for enriched local feature extraction. Experiments demonstrate ConeE's superior performance over state-of-the-art methods, offering an advanced solution for inductive KG reasoning.</p> -->
                        </div>
                    </div>
                </div>

                <!-- 论文3 -->
                <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract3.png" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract3.png'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">De-accumulated error collaborative learning framework for predicting Alzheimer's disease progression</div>
                        <div class="paper-authors">Hongli Cheng<sup>1</sup>, Shizhong Yuan<sup>1</sup>, Weimin Li*, Xiao Yu, <span class="highlight">Fangyu Liu</span>, Xiao Liu, Tsigabu Teame Bezabih (<span style="background-color: rgb(252, 248, 227);"><sup>1</sup> <i>equal contribution</i></span>)</div>
                        <div class="paper-journal"><b>Biomedical Signal Processing and Control</b>, 2024</div>
                        <div class="paper-extra">中科院 2 区, JCR 1 区, IF:4.9</div>
                        <div class="paper-links">
                            <a href="https://doi.org/10.1016/j.bspc.2023.105767" target="_blank">[论文]</a>
                        </div>
                        <div class="paper-abstract">
							<p>本研究提出LSTM-TSGAIN协同学习框架解决AD进展预测中的缺失数据问题，包含三大创新：1)生成对抗插补降低LSTM误差；2)对抗插补与时间序列模块协同训练；3)可变长度输入适应不同访视次数。基于ADNI 1256例纵向数据的实验验证其优越性，既解决了临床缺失数据难题，也为医学时间序列分析提供了新思路。</p>
							<!-- <p>This study proposes an innovative LSTM-TSGAIN collaborative learning framework to address missing data challenges in Alzheimer's disease progression prediction. The framework introduces three key innovations: 1) a generative adversarial imputation method to reduce LSTM prediction errors, 2) collaborative training between adversarial imputation and time-series learning modules, and 3) variable-length inputs accommodating differing subject visit frequencies. Experiments on longitudinal ADNI data from 1256 subjects demonstrate superior performance over state-of-the-art methods, offering both a solution to clinical missing data issues and novel insights for medical time-series analysis.</p> -->
                        </div>
                    </div>
                </div>

                <!-- 论文2 -->
                <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract2.png" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract2.png'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">MMTFN: Multi-modal Multi-scale transformer fusion network for Alzheimer's disease diagnosis</div>
                        <div class="paper-authors">Shang Miao<sup>#</sup>, Qun Xu<sup>#</sup>, Weimin Li*, Chao Yang*, Bin Sheng, <span class="highlight">Fangyu Liu</span>, Tsigabu T. Bezabih, Xiao Yu (<span style="background-color: rgb(252, 248, 227);"><sup>#</sup> <i>senior authors</i></span>)</div>
                        <div class="paper-journal"><b>International Journal of Imaging Systems and Technology</b>, 2024</div>
                        <div class="paper-extra">中科院 4 区, JCR 2 区, IF:3</div>
                        <div class="paper-links">
                            <a href="https://doi.org/10.1002/ima.22970" target="_blank">[论文]</a>
                        </div>
                        <div class="paper-abstract">
							<p>本研究针对阿尔茨海默病（AD）多模态神经影像融合的难题，创新性地提出了多模态多尺度Transformer融合网络（MMTFN）。该网络整合了3D多尺度残差块（3DMRB）和Transformer网络，前者提取多尺度局部病理特征，后者建模长程依赖关系并学习多模态联合表征。基于ADNI数据库720例受试者（含MRI和PET影像）的五项实验验证表明，该方法在AD与正常对照分类中达到94.61%的准确率，显著优于现有方法，为AD的计算机辅助诊断提供了新方案。</p>
							<!-- <p>This study proposes a novel Multi-modal Multi-scale Transformer Fusion Network (MMTFN) to address the challenges of fusing multi-modal neuroimaging data for Alzheimer's disease (AD) diagnosis. The network combines 3D Multi-scale Residual Blocks (3DMRB) for extracting local pathological features at different scales with a Transformer network for learning long-range dependencies and joint representations of multi-modal data. Evaluated through five experiments on 720 subjects from ADNI (using MRI and PET images), MMTFN achieved superior performance with 94.61% classification accuracy between AD and Normal Controls, establishing a new effective tool for computer-aided AD diagnosis.</p> -->
                        </div>
                    </div>
                </div>
                
                <!-- 论文1 -->
                <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract1.png" alt="论文图摘要" onerror="this.src='./graphical abstract/abstract1.png'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">Patch-based deep multi-modal learning framework for Alzheimer's disease diagnosis using multi-view neuroimaging</div>
                        <div class="paper-authors"><span class="highlight">Fangyu Liu</span><sup>1</sup>, Shizhong Yuan<sup>1</sup>, Weimin Li*, Qun Xu**, Bin Sheng (<span style="background-color: rgb(252, 248, 227);"><sup>1</sup> <i>equal contribution</i></span>)</div>
                        <div class="paper-journal"><b>Biomedical Signal Processing and Control</b>, 2023</div>
                        <div class="paper-extra">中科院 2 区, JCR 1 区, IF:4.9</div>
                        <div class="paper-links">
                            <a href="https://doi.org/10.1016/j.bspc.2022.104400" target="_blank">[论文]</a>
                        </div>
                        <div class="paper-abstract">
							<p>本研究针对阿尔茨海默病（AD）诊断中单模态方法的局限性以及传统图像块方法的空间信息丢失问题，提出了一种基于图像块的深度多模态学习框架（PDMML）。该框架采用无先验知识的判别性位置发现策略自动筛选病变区域，避免了传统解剖标志检测方法的专家依赖性和病灶漏检；通过在图像块层级融合多模态特征捕获多维度疾病表征，并联合学习局部图像块以保留空间结构信息，解决了图像块展平导致的信息损失问题。基于ADNI数据库842例受试者的实验验证表明，该方法在关键区域定位和疾病诊断方面均显著优于现有方法，为早期轻度认知障碍（MCI）的计算机辅助诊断提供了更优解决方案。</p>
                            <!-- <p>This study addresses the limitations of single-modality methods and spatial information loss in patch-based approaches for Alzheimer's disease (AD) diagnosis by proposing a Patch-based Deep Multi-Modal Learning (PDMML) framework. The framework features a prior-free discriminative location discovery strategy to automatically identify potential lesion areas, eliminating reliance on anatomical landmarks and expert experience. It innovatively integrates multimodal features at the patch level to capture comprehensive disease representations while preserving spatial information through joint patch learning, overcoming the flattening-induced information loss. Evaluated on 842 ADNI subjects, PDMML demonstrates superior performance in both discriminative region localization and brain disease diagnosis, offering a robust computer-aided solution for early mild cognitive impairment (MCI) detection.</p> -->
                        </div>
                    </div>
                </div>
            </div>
			<div class="row">
				<div class="col-md-12">
					<!-- <ol class="papers-list" id="grants5">
						<li>
							<b>Fangyu Liu</b>, Hao Wang, Weilin Zang, Ye Li, Fangmin Sun*. <u> Influencing Factors Mining and Modeling of Energy Expenditure in Running Based on Wearable Sensors</u>. <i><b> International Conference on Sports Technology and Performance Analysis</b>, 2024</i>. (ICSTPA 2024, <i><b>Accept</b></i>)[<a href="https://fangyuliu-2023.github.io/index.html#publications">Link</a>][<a href="team/activities/Poster2.pdf" target="_blank">Poster</a>]
						</li>
						<li>
							Ruijie Shao, Hao Wang, <b>Fangyu Liu</b>, Fangmin Sun*. <u> Gait Recognition Method Based on Wearable Sensor Information Fusion</u>. <i><b> The 7th International Conference on Biological Information and Biomedical Engineering</b>, 2024</i>. (BIBE 2024)[<a href="https://ieeexplore.ieee.org/abstract/document/10830681" target="_blank">Link</a>][<a href="team/activities/Poster1.pdf" target="_blank">Poster</a>][<a href="team/activities/BIBE 2024 - Most Influencial Paper Award.pdf" target="_blank">Award</a>][<a href="https://mp.weixin.qq.com/s/JQhbjR142JafRyYETr95fg" target="_blank">Promotion</a>]
						</li>
						<li>
							Jingchao Wang, Weimin Li*, Alex Munyole Luvembe, Xiao Yu, Xinyi Zhang, <b>Fangyu Liu</b>, Fangfang Liu, Hao Wang, Zhenhai Wang, Qun Jin. <u> Exploring multi-granularity contextual semantics for fully inductive knowledge graph completion</u>. <i><b> Expert Systems With Applications</b>, 2025</i>. (CCF-C, 中科院 1 区 TOP, JCR 1 区, IF:7.5)[<a href="https://doi.org/10.1016/j.eswa.2024.125407" target="_blank">Link</a>]
						</li>
						<li>
							Xiao Liu, Weimin Li*, Shang Miao, <b>Fangyu Liu</b>, Ke Han, Tsigabu Teame Bezabih. <u> HAMMF: Hierarchical attention-based multi-task and multi-modal fusion model for computer-aided diagnosis of Alzheimer's disease</u>. <i><b> Computers in Biology and Medicine</b>, 2024</i>. (中科院 2 区 TOP, JCR 1 区, IF:7)[<a href="https://doi.org/10.1016/j.compbiomed.2024.108564" target="_blank">Link</a>]
						</li>
						<li>
							<b>Fangyu Liu</b>, Shizhong Yuan, Weimin Li*, Qun Xu, Xing Wu, Ke Han*, Jingchao Wang, Shang Miao. <u> Multi-task joint learning network based on adaptive patch pruning for Alzheimer's disease diagnosis and clinical score prediction</u>. <i><b> Biomedical Signal Processing and Control</b>, 2024</i>. (中科院 2 区, JCR 1 区, IF:4.9)[<a href="https://doi.org/10.1016/j.bspc.2024.106398" target="_blank">Link</a>]
						</li>
						<li>
							Jingchao Wang, Weimin Li*, Fangfang Liu, Zhenhai Wang, Alex Munyole Luvembe, Qun Jin, Quanke Pan, <b>Fangyu Liu</b>. <u> ConeE: Global and local context-enhanced embedding for inductive knowledge graph completion</u>. <i><b> Expert Systems With Applications</b>, 2024</i>. (CCF-C, 中科院 1 区 TOP, JCR 1 区, IF:7.5)[<a href="https://doi.org/10.1016/j.eswa.2023.123116" target="_blank">Link</a>]
						</li>
						<li>
							Hongli Cheng, Shizhong Yuan, Weimin Li*, Xiao Yu, <b>Fangyu Liu</b>, Xiao Liu, Tsigabu Teame Bezabih. <u> De-accumulated error collaborative learning framework for predicting Alzheimer's disease progression</u>. <i><b> Biomedical Signal Processing and Control</b>, 2024</i>. (中科院 2 区, JCR 1 区, IF:4.9)[<a href="https://doi.org/10.1016/j.bspc.2023.105767" target="_blank">Link</a>]
						</li>
						<li>
							Shang Miao, Qun Xu, Weimin Li*, Chao Yang*, Bin Sheng, <b>Fangyu Liu</b>, Tsigabu T. Bezabih, Xiao Yu. <u> MMTFN: Multi-modal Multi-scale transformer fusion network for Alzheimer's disease diagnosis</u>. <i><b> International Journal of Imaging Systems and Technology</b>, 2024</i>. (中科院 4 区, JCR 2 区, IF:3)[<a href="https://doi.org/10.1002/ima.22970" target="_blank">Link</a>]
						</li>
						<li>
							<b>Fangyu Liu</b>, Shizhong Yuan, Weimin Li*, Qun Xu**, Bin Sheng. <u> Patch-based deep multi-modal learning framework for Alzheimer's disease diagnosis using multi-view neuroimaging</u>. <i><b> Biomedical Signal Processing and Control</b>, 2023</i>. (中科院 2 区, JCR 1 区, IF:4.9)[<a href="https://doi.org/10.1016/j.bspc.2022.104400" target="_blank">Link</a>]
						</li>
						<li>
							Jingchao Wang, Weimin Li*, Fangfang Liu, Zhenhai Wang, Alex Munyole Luvembe, Qun Jin, Quanke Pan, <b>Fangyu Liu</b>. <u> ConeE: Global and Local Context-enhanced Embedding for Inductive Knowledge Graph Completion</u>. <i><b> Expert Systems With Applications</b>, 2023</i>. (中科院 1 区, JCR 1 区, IF:8.5, <i><b>Accept</b></i>)[<a href="https://fangyuliu-2023.github.io/index.html#publications">Link</a>]
						</li>
					</ol> -->
					<ol class="jump-list" id="grants6">
                        <li>
                            <b><a href="paper.html">更多详情...</a></b>
                        </li>
                    </ol>
				</div>
			</div>

			<h2 id="activities" class="page-header">学术服务</h2>
			<div class="row">
				<div class="col-md-12">
<!--					<ul class="activities-list" id="grants7">-->
<!--						<li>XXX人工智能大会</li>-->
<!--					</ul>-->
<!--					<h4><b>编委会</b></h4>-->
<!--					<ul class="editor-list" id="grants8">-->
<!--						<li>杂志xxxx的Guest Editor</li>-->
<!--						<li>杂志xxxx的Guest Editor</li>-->
<!--						<li>杂志xxxx的Guest Editor</li>-->
<!--					</ul>-->
					<h4><b>审稿人</b></h4>
					<ul class="reviewer-list" id="grants9">
						<li>IEEE Journal of Biomedical and Health Informatics期刊的审稿人</li>
						<li>Knowledge-Based Systems期刊的审稿人</li>
						<li>Scientific Reports期刊的审稿人</li>
						<li>The 21st IEEE International Conference on Ubiquitous Intelligence and Computing (UIC 2024)会议的审稿人</li>
					</ul>
					<h4><b>志愿者</b></h4>
					<ul class="chair-list" id="grants10">
						<li>The 15th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM BCB 2024)会议的志愿者</li>
						<li>“中国科学院第二十届公众科学日”大型公益科普活动志愿者</li>
					</ul>
<!--					<h4><b>程序委员会主席</b></h4>-->
<!--					<ul class="chair-list" id="grants10">-->
<!--						<li>国际会议xxxx的程序主席</li>-->
<!--						<li>国际会议xxxx的程序主席成员</li>-->
<!--						<li>国际会议xxxx的Chair</li>-->
<!--					</ul>-->
<!--					<h4><b>组织者</b></h4>-->
<!--					<ul class="organizer-list" id="grants11">-->
<!--						<li>国际会议xxxx的的组织者</li>-->
<!--					</ul>-->
				</div>
			</div>

			<!--
			<h2 id="teaching" class="page-header">教授课程</h2>
			<div class="row">
				<div class="col-md-12">
					<ul class="teach-list" id="grants12">
						<li>炼丹</li>
						<li>符箓</li>
						<li>法器</li>
					</ul>
				</div>
			</div>
			-->
			<!--
			<h2 id="activity" class="page-header">邀请报告</h2>
			<div class="row">
				<div class="col-md-12">
					<ol class="paper-list" id="grants13">
						<li>
							<b>题目:</b> <u></u> [<a href="./supplement/.pdf" target="_blank">Slide</a>]<br/>
							<b>地点:</b> <br/>
							<b>时间:</b> 2018年10月19日
						</li>
						<li>
							<b>题目:</b> <u></u> [<a href="./supplement/.pdf" target="_blank">Slide</a>]<br/>
							<b>地点:</b> <br/>
							<b>时间:</b> 2018年9月19日
						</li>
					</ol>
				</div>
			</div>
			-->
			<!--
			<h2 id="talks" class="page-header">参与活动</h2>
			<div class="row">
				<div class="col-md-12">
					<h4><b>审稿人</b></h4>
					<ul class="paper-list" id="grants14">
						  <li></li>
					</ul>
					<h4><b>志愿者</b></h4>
					<ul class="paper-list" id="grants15">
						  <li></li>
						  <li></li>
						  <li></li>
					</ul>
					<h4><b>访问</b></h4>
					<ul class="paper-list" id="grants16">
						  <li>2017年10月-12月。应xx大学教授邀请，访问xx大学</a></li>
						  <li>2016年4月-7月。 应华为xxx实验室项目经理xxx邀请访问XX技术有限公司。</li>
					</div>
			</div>
			-->
			<h2 id="award" class="page-header">荣誉奖项</h2>
			<div class="row">
				<div class="col-md-12">
					<ul class="award-list" id="grants17">
<!--						<li>2022-2023, 上海大学年度“二等学业奖学金”</li>-->
<!--						<li>2021-2022, 上海大学年度“二等学业奖学金”</li>-->
<!--						<li>2021-2022, 上海大学计算机工程与科学学院“优秀党支部”</li>-->
<!--						<li>2021-2022, 上海大学研究生党支部“十佳主题党日”</li>-->
<!--						<li>2021, 上海大学研究生暑期社会实践校级优秀项目</li>-->
<!--						<li>2020-2021, 上海大学年度“二等学业奖学金”</li>-->
<!--						<li>2020-2021, 上海大学研究生团支部学生先进集体</li>-->
<!--						<li>2020-2021, 学生社区“积极分子”</li>-->
						<li>2020, 湖南省优秀毕业生</li>
						<li>2020, 南华大学2020届优秀毕业生</li>
<!--						<li>2018-2019, 南华大学年度“一等奖学金”</li>-->
<!--						<li>2018-2019, 南华大学年度“优秀学生干部”</li>-->
<!--						<li>2019, 第十二届中国大学生计算机设计大赛中南地区三等奖</li>-->
<!--						<li>2019, 第十六届五一数学建模竞赛二等奖</li>-->
<!--						<li>2019, 第十届蓝桥杯湖南省C/C++程序设计大学B组二等奖</li>-->
<!--						<li>2018, 第一届“全国高校绿色计算大赛”（开源标注组）全国一等奖</li>-->
<!--						<li>2017-2018, 国家励志奖学金</li>-->
<!--						<li>2017-2018, 南华大学年度“优秀共青团干部”</li>-->
<!--						<li>2017-2018, 南华大学年度“三好学生”</li>-->
<!--						<li>2016-2017, 南华大学年度“一等奖学金”</li>-->
<!--						<li>2016-2017, 南华大学年度“优秀学生干部”</li>-->
<!--						<li>2016-2017, 南华大学年度“三好学生”</li>-->
<!--						<li>2017, 南华大学第十三届ACM程序设计大赛一等奖</li>-->
<!--						<li>2016, 南华大学“学法、守法、用法”小品竞赛一等奖</li>-->
<!--						<li>2015, 高中阶段“三好学生”</li>-->
					</ul>
				</div>
			</div>
		</div>
	</div>
</div>


<footer class="footer">
	<div class="container">
		<p class="text-muted">Homepage Create:09/2023, Last Modified:04/2025, Copyright © Fangyu Liu 2023-2025</p>
	</div>
	<div class="color_wrapper"></div>
</footer>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
	<script src="./Javascript/jquery.min.js"></script>
    <script src="./Javascript/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 10 bug -->
<!--    <script src="js/ie10-viewport-bug-workaround.js"></script>-->
	<!-- 为按钮添加点击事件 -->
	<script>
		$(document).ready(function(){
		  // 监听窗口宽度变化
		  $(window).resize(function() {
			if ($(window).width() < 768) { // 假设小屏的阈值为768px
			  // 先移除之前的事件监听器，预防尺寸变化重复绑定监听事件
			  $(".navbar-toggle").off('click');
			  $('.navbar-collapse a').off('click');

			  // 监听导航栏按钮的点击事件
			  $(".navbar-toggle").click(function(){
				$(".bs-navbar-collapse").toggleClass("in");
				$(this).toggleClass("collapsed"); // 切换按钮的collapsed类
			  });

			  // 监听导航栏链接的点击事件
			  $('.navbar-collapse a').on('click', function () {
				  $(".navbar-toggle").click(); // 触发导航栏按钮的点击事件，使导航栏收缩
			  });
			}
		  }).trigger('resize'); // 初始触发一次resize事件以应用事件监听器
		});
	</script>
  </body>

</html>