<!--
create: Fangyu Liu
date:April 2021
-->
<!DOCTYPE html>
<html lang="en">
  <head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- Meta, title, CSS, favicons, etc. -->
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="">
<meta name="keywords" content="">
<meta name="author" content="Fangyu Liu">

<title>Fangyu Liu</title>

<!-- Bootstrap core CSS -->
<link href="./css/bootstrap.min.css" rel="stylesheet">
<link href="./css/custom.css" rel="stylesheet">
<link href="./css/syntax.css" rel="stylesheet">
<link rel="icon" type="image/png" href="icon/bachelor.png">
<!--[if lt IE 9]>-->
<!--	<script src="../static/js/ie8-responsive-file-warning.js"></script>-->
<!--<![endif]-->

<!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
<!--[if lt IE 9]>
  <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
  <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->

  <style type="text/css"></style>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-9372397-3', 'auto');
  ga('send', 'pageview');

</script>
</head>
  <body data-twttr-rendered="true">
      <header class="navbar navbar-static-top bs-docs-nav custom_navbar navbar-fixed-top" id="top" role="banner">
    <div class="color_wrapper"></div>
  <div class="container">
    <div class="navbar-header">
      <button class="navbar-toggle collapsed" type="button" data-bs-toggle="collapse" data-bs-target=".bs-navbar-collapse">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav class="collapse navbar-collapse bs-navbar-collapse">
      <ul class="nav navbar-nav navbar-right">
        <li><a href="#about">Biography</a></li>
		<li><a href="#research">Interests</a></li>
		<li><a href="#education">Education</a></li>
		<li><a href="#work">Working</a></li>
		<li><a href="#projects">Projects</a></li>
        <li><a href="#publications">Publications</a></li>
		<li><a href="./team/activity_english.html">Activities</a></li>
		<li><a href="#activities">Service</a></li>
		<!--
		<li><a href="#talks">Talks</a></li>
        <li><a href="#activity">Activities</a></li>
		<li><a href="#award">Awards</a></li>
		-->
		<li><a href="#award">Awards</a></li>
		<li><a href="index.html">中文版本</a></li>
      </ul>
    </nav>
  </div>
</header>
<div class="nav-space"></div>
  <div class="page-titile-wraper aboutme-wraper">
  <div class="container">
    <div class="row">
      <div class="col-md-3"><img src="assets/fangyu_liu.jpg" style= "margin-top:20px" class="img-responsive img-circle"></div>
      <div class="col-md-9">
		<br>
		<div style="display: inline;">
			<img src="icon/bachelor.png" width="30px" height="35px" style="float:left;margin-top: 15px;margin-right: 10px;"/>
        	<h3 id="about" style="position: relative;"><b>Fangyu Liu [刘芳宇]</b></h3>
		</div>
		<div style="display: inline;">
			<img src="icon/hat.png" width="30px" height="30px" style="float:left;margin-top: 10px;margin-right: 10px;"/>
        	<h3 style="position: relative;">M.S.</h3>
		</div>
		<div style="display: inline;">
			<img src="icon/team.png" width="30px" height="30px" style="float:left;margin-right: 10px;"/>
			<h4 style="position: relative;margin-top: 15px;"><a href="./team/activity_english.html">Academic Activities</a></h4>
		</div>
		<div style="display: inline;">
			<img src="icon/profession.png" width="30px" height="30px" style="float:left;margin-right: 10px;"/>
			<h4 style="position: relative;margin-top: 15px;">Ph.D. Candidate at <a href="https://www.siat.ac.cn/" target="_blank">Shenzhen Institutes of Advanced Technology</a>, <a href="https://www.ucas.ac.cn/" target="_blank">University of Chinese Academy of Sciences</a></h4>
		</div>
		<br>
		<div style="display: inline;">
			<img src="icon/office.png" width="30px" height="35px" style="float:left;margin-top: -23px;margin-right: 10px;"/>
			<h4 style="position: relative;margin-top: -13px;">1068 Xueyuan Avenue, Shenzhen University Town, Shenzhen, P.R.China, Zip code: 518055</h4>
		</div>
		<!--<h4>电话: +86 130, +3 69</h4>-->
		<div style="display: inline;">
			<img src="icon/email.png" width="30px" height="20px" style="float:left;margin-top: 5px;margin-right: 10px;"/>
			<h4 style="position: relative;margin-top: 15px;">fy dot liu1 at siat dot ac dot cn</h4>
		</div>
		<br>
		<div style="display: inline;">
			<img src="icon/click1.png" width="30px" height="30px" style="float:left;margin-top: -23px;margin-right: 10px;"/>
			<h4 style="position: relative;margin-top: -15px;"><a href="./index.html">中文主页</a>, <a href="https://scholar.google.com/citations?user=dR2OCZ8AAAAJ">Google Scholar</a>, <a href="https://orcid.org/0000-0001-8332-0154">ORCID</a></h4>
		</div>
		<br>
        <!--<div class="pull-right">
          <a href="#">li1989 {at} mail.dlut.edu.cn</a>
          <a href="./assets/_cv.pdf" target="_blank" class="btn btn-primary btn-md" role="button">
            <span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span>Download my CV
          </a>
        </div>-->
      </div>
    </div>
  </div>
</div>

<div class="container">
	<div class="row">
		<div class="col-md-12" role="main">

			<h2 id="biography" class="page-header">Biography</h2>
			<div class="row">
				<div class="col-md-12">
					<ul class="biography-list" id="grants0">
						<li>Fangyu Liu received the B.S. degree in software engineering from the University of South China, Hengyang, China, in 2020, and the M.S. degree in computer application technology from Shanghai University, Shanghai, China, in 2023. He is currently pursuing the Ph.D. degree in computer application technology at the University of Chinese Academy of Sciences, Beijing, China. His current research interests include biomedical signal processing, sensor information fusion, wearable health-monitoring devices, medical image analysis, and machine learning.</li>
					</ul>
				</div>
			</div>

			<h2 id="research" class="page-header">Research Interests</h2>
			<div class="row">
				<div class="col-md-12">
					<ul class="research-list" id="grants1">
						<li>Biomedical Signal Processing</li>
						<li>Sensor Information Fusion</li>
						<li>Wearable Health-monitoring Devices</li>
<!--						<li>Wearable Computing</li>-->
<!--						<li>Data Fusion Multiple Sensor</li>-->
						<li>Medical Image Analysis</li>
						<li>Machine Learning</li>
					</ul>
				</div>
			</div>

			<h2 id="education" class="page-header">Education</h2>
			<div class="row">
				<div class="col-md-12">
					<ul class="educate-list-3" id="grants2-3">
						<li>2023.09 - Present: Ph.D., Computer Application Technology, <a href="https://www.siat.ac.cn/" target="_blank">Shenzhen Institutes of Advanced Technology</a>, <a href="https://www.ucas.ac.cn/" target="_blank">University of Chinese Academy of Sciences</a></li>
					</ul>
					<ul class="educate-list-2" id="grants2-2">
						<li>2020.09 - 2023.06: M.S., Computer Application Technology, <a href="https://cs.shu.edu.cn/" target="_blank">School of Computer Engineering and Science</a>, <a href="https://www.shu.edu.cn/" target="_blank">Shanghai University</a> (Recommended Exam-exempted Post-graduate)</li>
					</ul>
					<ul class="educate-list-1" id="grants2-1">
						<li>2016.09 - 2020.06: B.S., Software Engineering, <a href="https://jsjxy.usc.edu.cn/" target="_blank">School of Computer / School of Software</a>, <a href="https://www.usc.edu.cn/" target="_blank">University of South China</a> (Outstanding Engineer Education and Training Program)</li>
					</ul>
				</div>
			</div>

			<h2 id="work" class="page-header">Working Experience</h2>
			<div class="row">
				<div class="col-md-12">
					<ul class="work-list-2" id="grants3-2">
						<li>2023.09.19 - 2024.05.07：A student co-trained by <a href="https://www.bgi.com/" target="_blank">BGI-shenzhen</a></li>
					</ul>
					<ul class="work-list-1" id="grants3-1">
						<li>2019.11.15 - 2020.02.15：Intern at Nuclear Industry Engineering Research and Design Co., LTD</li>
					</ul>
				</div>
			</div>

			<h2 id="projects" class="page-header">Projects</h2>
			<div class="row">
				<div class="col-md-12">
					<ol class="project-list" id="grants4">
						<li>
							<b>Topic:</b> <u>Research on wearable intelligent sensing and computing methods for individualized sports health</u> [<a href="https://fangyuliu-2023.github.io/index_english.html#projects">Link</a>]<br/>
							<b>Source:</b> Shenzhen International Cooperation Project<br/>
							<!-- <b>Grant Number:</b> GJHZ20220913142808016<br/> -->
							<b>Date:</b> 2023 - 2025<br/>
							<b>Duty:</b> Student participation
						</li>
					</ol>
				</div>
			</div>
			<div class="row">
				<div class="col-md-12">
					<ol class="project-list" id="grants4">
						<li>
							<b>Topic:</b> <u>Tensor-based multimodal data fusion with causal reasoning and interpretable aid diagnosis for Alzheimer's disease</u> [<a href="https://fangyuliu-2023.github.io/index_english.html#projects">Link</a>]<br/>
							<b>Source:</b> Key Program for International Science and Technology Cooperation Projects of China<br/>
							<!-- <b>Grant Number:</b> 2024YFE0102100<br/> -->
							<b>Date:</b> 2024 - Present<br/>
							<b>Duty:</b> Student participation
						</li>
					</ol>
				</div>
			</div>

			<h2 id="publications" class="page-header">Recent Publications [<a href="paper_english.html">All</a>]</h2>
            <div class="papers-container">
                <!-- 论文9 -->
                <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract9.png" alt="Graphical Abstract" onerror="this.src='./graphical abstract/abstract9.png'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">Influencing Factors Mining and Modeling of Energy Expenditure in Running Based on Wearable Sensors</div>
                        <div class="paper-authors"><span class="highlight">Fangyu Liu</span>, Hao Wang, Weilin Zang, Ye Li, Fangmin Sun*</div>
                        <div class="paper-journal"><b>International Conference on Sports Technology and Performance Analysis</b>, 2024</div>
                        <div class="paper-extra">ICSTPA 2024, EI</div>
                        <div class="paper-links">
                            <a href="https://doi.org/10.1145/3723936.3723943" target="_blank">[Paper]</a>
                            <a href="team/activities/Poster2.pdf" target="_blank">[Poster]</a>
                        </div>
                        <div class="paper-abstract">
                            <!-- <p>本研究针对基于深度学习的可穿戴能耗监测方法可解释性不足的问题，开发了基于可解释回归算法的跑步能耗实时预测模型。通过从人口统计、身体活动和生理指标三维度分析特征，创新提出手工特征选择方法筛选出743个关键特征。在多种机器学习算法中，梯度提升树（GBR）表现最优（CC=0.970，RMSE=1.004，MAE=0.729）。该研究不仅实现了高精度实时能耗预测，其"手工特征+可解释算法"的建模思路也为运动监测领域提供了新范式。</p> -->
                            <p>This study addresses the interpretability limitations of deep learning-based wearable energy expenditure monitoring by developing an explainable regression model for real-time running energy prediction. Through systematic analysis of demographic, physical activity and physiological features, the research proposes a novel hand-crafted feature selection method identifying 743 key features. Among various machine learning algorithms tested, Gradient Boosted Regression (GBR) achieved optimal performance (CC=0.970, RMSE=1.004, MAE=0.729) in five-fold cross-validation with 34 volunteers. The study not only accomplishes accurate real-time energy expenditure prediction but also provides a new technical approach combining manual feature engineering with interpretable algorithms for sports monitoring applications.</p>
                        </div>
                    </div>
                </div>
                
                <!-- 论文8 -->
                <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract8.jpg" alt="Graphical Abstract" onerror="this.src='./graphical abstract/abstract8.jpg'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">Gait Recognition Method Based on Wearable Sensor Information Fusion</div>
                        <div class="paper-authors">Ruijie Shao, Hao Wang, <span class="highlight">Fangyu Liu</span>, Fangmin Sun*</div>
                        <div class="paper-journal"><b>The 7th International Conference on Biological Information and Biomedical Engineering</b>, 2024</div>
                        <div class="paper-extra">BIBE 2024, EI</div>
                        <div class="paper-links">
                            <a href="https://ieeexplore.ieee.org/abstract/document/10830681" target="_blank">[Paper]</a>
                            <a href="team/activities/Poster1.pdf" target="_blank">[Poster]</a>
                            <a href="team/activities/BIBE 2024 - Most Influencial Paper Award.pdf" target="_blank">[Award]</a>
                            <a href="https://mp.weixin.qq.com/s/JQhbjR142JafRyYETr95fg" target="_blank">[Promotion]</a>
                        </div>
                        <div class="paper-abstract">
                            <!-- <p>本研究针对实际步态识别系统中的穿戴设备资源限制和用户体验问题，提出了一种基于多传感器融合的创新方法。通过设计包含注意力机制的轻量化网络模型，在减少参数量的同时提升识别精度。研究系统比较了身体不同部位的传感器性能，确定了最佳个性化配置方案，并通过实验验证数据级融合优于决策级融合。探索了多种身体传感器组合，提出了最优配置方案。通过分析识别规模对模型的影响，证实了该方法的高效数据处理能力和鲁棒性，为实用化步态识别系统开发提供了重要技术支持。</p> -->
                            <p>This study addresses the challenges of wearable device resource limitations and user experience in practical gait recognition systems by proposing an innovative multi-sensor fusion-based approach. A lightweight network model incorporating an attention mechanism was designed to reduce parameters while improving accuracy. The research systematically compared sensor performance at different body locations to identify optimal individualized configurations, experimentally validating data-level fusion's superiority over decision-level fusion. Various body-worn sensor combinations were investigated to determine the optimal setup. Analysis of subject scale impact confirmed the method's efficient data processing capabilities and robustness, providing important technical support for developing practical gait recognition systems.</p>
                        </div>
                    </div>
                </div>

                <!-- 论文7 -->
				<div class="paper-item">
					<div class="paper-image">
						<img src="./graphical abstract/abstract7.png" alt="Graphical Abstract" onerror="this.src='./graphical abstract/abstract7.png'">
					</div>
					<div class="paper-info">
						<div class="paper-title">Exploring multi-granularity contextual semantics for fully inductive knowledge graph completion</div>
						<div class="paper-authors">Jingchao Wang, Weimin Li*, Alex Munyole Luvembe, Xiao Yu, Xinyi Zhang, <span class="highlight">Fangyu Liu</span>, Fangfang Liu, Hao Wang, Zhenhai Wang, Qun Jin</div>
						<div class="paper-journal"><b>Expert Systems With Applications</b>, 2025</div>
						<div class="paper-extra">CCF-C, Q1 of CAS, Q1 of JCR, IF:7.5</div>
						<div class="paper-links">
							<a href="https://doi.org/10.1016/j.eswa.2024.125407" target="_blank">[Paper]</a>
						</div>
						<div class="paper-abstract">
							<!-- <p>本研究提出多粒度上下文语义框架MGCS解决全归纳KGC任务，通过路径建模网络（PMN）的创新路径转换策略和子图建模网络（SMN）的高阶语义提取能力，结合对比学习和概念增强编码，显著提升了未见实体/关系的推理性能。实验证明该框架通过比较目标三元组与相似案例的子图语义，实现了更精准的全归纳知识图谱补全。</p> -->
							<p>This study proposes MGCS, a Multi-Granularity Contextual Semantic framework for fully inductive KGC, addressing limitations in path transformation and high-order semantic utilization. The framework features: (1) a Path Modeling Network (PMN) with innovative conversion strategies to enhance PLMs' path understanding and reliability filtering; (2) a Subgraph Modeling Network (SMN) using interactive GNNs and concept-enhanced encoding to capture high-order semantics via contrastive learning. By comparing subgraph semantics between target triplets and similar cases, MGCS achieves superior performance in handling unseen entities/relations, as validated on benchmark datasets.</p>
						</div>
					</div>
				</div>
              
                <!-- 论文6 -->
            	<div class="paper-item">
					<div class="paper-image">
						<img src="./graphical abstract/abstract6.png" alt="Graphical Abstract" onerror="this.src='./graphical abstract/abstract6.png'">
					</div>
					<div class="paper-info">
						<div class="paper-title">HAMMF: Hierarchical attention-based multi-task and multi-modal fusion model for computer-aided diagnosis of Alzheimer's disease</div>
						<div class="paper-authors">Xiao Liu, Weimin Li*, Shang Miao, <span class="highlight">Fangyu Liu</span>, Ke Han, Tsigabu Teame Bezabih</div>
						<div class="paper-journal"><b>Computers in Biology and Medicine</b>, 2024</div>
						<div class="paper-extra">Q2 of CAS, Q1 of JCR, IF:7</div>
						<div class="paper-links">
							<a href="https://doi.org/10.1016/j.compbiomed.2024.108564" target="_blank">[Paper]</a>
						</div>
						<div class="paper-abstract">
							<!-- <p>本研究提出的HAMMF模型通过上下文分层注意力模块（CHAM）从多模态影像中提取特征，结合多任务学习框架，在ADNI 720例数据上实现93.15%的分类准确率。该方法创新性地采用通道-空间双重注意力机制和Transformer特征关联，在保证诊断精度的同时优化模型部署效率，为AD临床诊断提供了高效解决方案。</p> -->
							<p>This study proposes HAMMF, a Hierarchical Attention-based Multi-task Multi-modal Fusion model for Alzheimer's diagnosis, addressing challenges of model complexity in clinical deployment. The innovative Contextual Hierarchical Attention Module (CHAM) extracts fine-grained features from MRI/PET data via channel-spatial attention mechanisms, while Transformer captures cross-modal correlations. Multi-task learning jointly optimizes AD classification, cognitive score regression and age prediction. Evaluated on 720 ADNI subjects, HAMMF achieves 93.15% AD/NC classification accuracy with demonstrated pathological feature discernment, offering both diagnostic precision and clinical applicability.</p>
						</div>
					</div>
                </div>
                
                <!-- 论文5 -->
                <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract5.png" alt="Graphical Abstract" onerror="this.src='./graphical abstract/abstract5.png'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">Multi-task joint learning network based on adaptive patch pruning for Alzheimer's disease diagnosis and clinical score prediction</div>
                        <div class="paper-authors"><span class="highlight">Fangyu Liu</span><sup>1</sup>, Shizhong Yuan<sup>1</sup>, Weimin Li*, Qun Xu, Xing Wu, Ke Han*, Jingchao Wang, Shang Miao (<span style="background-color: rgb(252, 248, 227);"><sup>1</sup> <i>equal contribution</i></span>)</div>
                        <div class="paper-journal"><b>Biomedical Signal Processing and Control</b>, 2024</div>
                        <div class="paper-extra">Q2 of CAS, Q1 of JCR, IF:4.9</div>
                        <div class="paper-links">
                            <a href="https://doi.org/10.1016/j.bspc.2024.106398" target="_blank">[Paper]</a>
                        </div>
                        <div class="paper-abstract">
                            <!-- <p>本研究针对脑部疾病诊断和临床评分预测中的关键问题，提出了一种创新的多任务联合学习网络（MTJLN）。该方法通过将脑图像划分为216个覆盖所有潜在病变区域的局部图像块，并采用图像块剪枝算法自动筛选信息丰富区域，克服了传统方法需要预先确定判别性位置的局限。创新地在中间层融合基于图像块的细粒度多模态特征和粗粒度非图像特征，充分利用了多模态信息与多任务变量间的内在关联。特别设计的加权损失函数使不完整临床评分的受试者也能参与训练，显著提高了数据利用率。基于ADNI数据库842名受试者的实验验证了该方法在病理分期和临床评分预测方面的有效性，为脑部疾病的精准诊断和进展评估提供了新方案。</p> -->
                            <p>This study proposes an innovative Multi-Task Joint Learning Network (MTJLN) to address key challenges in brain disease diagnosis and clinical score prediction. The method divides brain images into 216 local patches covering all potential lesion areas and employs a patch pruning algorithm to automatically select informative regions, overcoming the limitations of pre-determining discriminative locations. The novel framework integrates fine-grained patch-based multimodal features with coarse-grained non-image features at intermediate layers, effectively utilizing intrinsic correlations between multimodal data and multitask variables. A specially designed weighted loss function enables the inclusion of subjects with incomplete clinical scores, significantly improving data utilization. Experiments on 842 ADNI subjects demonstrate the method's effectiveness in predicting pathological stages and clinical scores, providing a new solution for precise brain disease diagnosis and progression assessment.</p>
                        </div>
                    </div>
                </div>

                <!-- 论文4 -->
                <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract4.png" alt="Graphical Abstract" onerror="this.src='./graphical abstract/abstract4.png'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">ConeE: Global and local context-enhanced embedding for inductive knowledge graph completion</div>
                        <div class="paper-authors">Jingchao Wang, Weimin Li*, Fangfang Liu, Zhenhai Wang, Alex Munyole Luvembe, Qun Jin, Quanke Pan, <span class="highlight">Fangyu Liu</span></div>
                        <div class="paper-journal"><b>Expert Systems With Applications</b>, 2024</div>
                        <div class="paper-extra">CCF-C, Q1 of CAS, Q1 of JCR, IF:7.5</div>
                        <div class="paper-links">
                            <a href="https://doi.org/10.1016/j.eswa.2023.123116" target="_blank">[Paper]</a>
                        </div>
                        <div class="paper-abstract">
							<!-- <p>本研究提出知识图谱补全（KGC）的新型框架ConeE，通过全局上下文建模模块（GCMM）结合BERT编码器与对比学习提取全局语义，以及局部上下文建模模块（LCMM）采用交互式图神经网络和互信息最大化捕获局部特征，有效解决了归纳式场景下现有方法语义利用不足和稀疏子图性能差的问题。实验验证其显著优于现有最优方法，为知识图谱的归纳推理提供了创新性解决方案。</p> -->
							<p>This study proposes ConeE, a global and local context-enhanced embedding network for knowledge graph completion (KGC), addressing limitations of existing methods in inductive settings where test entities are unseen during training. The framework features: (1) a Global Context Modeling Module (GCMM) with BERT-based encoder and contrastive learning to extract global semantics, plus a dual-perspective scoring network; (2) a Local Context Modeling Module (LCMM) using interactive GNNs and mutual information maximization for enriched local feature extraction. Experiments demonstrate ConeE's superior performance over state-of-the-art methods, offering an advanced solution for inductive KG reasoning.</p>
                        </div>
                    </div>
                </div>

                <!-- 论文3 -->
                <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract3.png" alt="Graphical Abstract" onerror="this.src='./graphical abstract/abstract3.png'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">De-accumulated error collaborative learning framework for predicting Alzheimer's disease progression</div>
                        <div class="paper-authors">Hongli Cheng<sup>1</sup>, Shizhong Yuan<sup>1</sup>, Weimin Li*, Xiao Yu, <span class="highlight">Fangyu Liu</span>, Xiao Liu, Tsigabu Teame Bezabih (<span style="background-color: rgb(252, 248, 227);"><sup>1</sup> <i>equal contribution</i></span>)</div>
                        <div class="paper-journal"><b>Biomedical Signal Processing and Control</b>, 2024</div>
                        <div class="paper-extra">Q2 of CAS, Q1 of JCR, IF:4.9</div>
                        <div class="paper-links">
                            <a href="https://doi.org/10.1016/j.bspc.2023.105767" target="_blank">[Paper]</a>
                        </div>
                        <div class="paper-abstract">
							<!-- <p>本研究提出LSTM-TSGAIN协同学习框架解决AD进展预测中的缺失数据问题，包含三大创新：1)生成对抗插补降低LSTM误差；2)对抗插补与时间序列模块协同训练；3)可变长度输入适应不同访视次数。基于ADNI 1256例纵向数据的实验验证其优越性，既解决了临床缺失数据难题，也为医学时间序列分析提供了新思路。</p> -->
							<p>This study proposes an innovative LSTM-TSGAIN collaborative learning framework to address missing data challenges in Alzheimer's disease progression prediction. The framework introduces three key innovations: 1) a generative adversarial imputation method to reduce LSTM prediction errors, 2) collaborative training between adversarial imputation and time-series learning modules, and 3) variable-length inputs accommodating differing subject visit frequencies. Experiments on longitudinal ADNI data from 1256 subjects demonstrate superior performance over state-of-the-art methods, offering both a solution to clinical missing data issues and novel insights for medical time-series analysis.</p>
                        </div>
                    </div>
                </div>
                
                <!-- 论文2 -->
                <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract2.png" alt="Graphical Abstract" onerror="this.src='./graphical abstract/abstract2.png'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">MMTFN: Multi-modal Multi-scale transformer fusion network for Alzheimer's disease diagnosis</div>
                        <div class="paper-authors">Shang Miao<sup>#</sup>, Qun Xu<sup>#</sup>, Weimin Li*, Chao Yang*, Bin Sheng, <span class="highlight">Fangyu Liu</span>, Tsigabu T. Bezabih, Xiao Yu (<span style="background-color: rgb(252, 248, 227);"><sup>#</sup> <i>senior authors</i></span>)</div>
                        <div class="paper-journal"><b>International Journal of Imaging Systems and Technology</b>, 2024</div>
                        <div class="paper-extra">Q4 of CAS, Q2 of JCR, IF:3</div>
                        <div class="paper-links">
                            <a href="https://doi.org/10.1002/ima.22970" target="_blank">[Paper]</a>
                        </div>
                        <div class="paper-abstract">
							<!-- <p>本研究针对阿尔茨海默病（AD）多模态神经影像融合的难题，创新性地提出了多模态多尺度Transformer融合网络（MMTFN）。该网络整合了3D多尺度残差块（3DMRB）和Transformer网络，前者提取多尺度局部病理特征，后者建模长程依赖关系并学习多模态联合表征。基于ADNI数据库720例受试者（含MRI和PET影像）的五项实验验证表明，该方法在AD与正常对照分类中达到94.61%的准确率，显著优于现有方法，为AD的计算机辅助诊断提供了新方案。</p> -->
							<p>This study proposes a novel Multi-modal Multi-scale Transformer Fusion Network (MMTFN) to address the challenges of fusing multi-modal neuroimaging data for Alzheimer's disease (AD) diagnosis. The network combines 3D Multi-scale Residual Blocks (3DMRB) for extracting local pathological features at different scales with a Transformer network for learning long-range dependencies and joint representations of multi-modal data. Evaluated through five experiments on 720 subjects from ADNI (using MRI and PET images), MMTFN achieved superior performance with 94.61% classification accuracy between AD and Normal Controls, establishing a new effective tool for computer-aided AD diagnosis.</p>
                        </div>
                    </div>
                </div>

                <!-- 论文1 -->
                <div class="paper-item">
                    <div class="paper-image">
                        <img src="./graphical abstract/abstract1.png" alt="Graphical Abstract" onerror="this.src='./graphical abstract/abstract1.png'">
                    </div>
                    <div class="paper-info">
                        <div class="paper-title">Patch-based deep multi-modal learning framework for Alzheimer's disease diagnosis using multi-view neuroimaging</div>
                        <div class="paper-authors"><span class="highlight">Fangyu Liu</span><sup>1</sup>, Shizhong Yuan<sup>1</sup>, Weimin Li*, Qun Xu**, Bin Sheng (<span style="background-color: rgb(252, 248, 227);"><sup>1</sup> <i>equal contribution</i></span>)</div>
                        <div class="paper-journal"><b>Biomedical Signal Processing and Control</b>, 2023</div>
                        <div class="paper-extra">Q2 of CAS, Q1 of JCR, IF:4.9</div>
                        <div class="paper-links">
                            <a href="https://doi.org/10.1016/j.bspc.2022.104400" target="_blank">[Paper]</a>
                        </div>
                        <div class="paper-abstract">
							<!-- <p>本研究针对阿尔茨海默病（AD）诊断中单模态方法的局限性以及传统图像块方法的空间信息丢失问题，提出了一种基于图像块的深度多模态学习框架（PDMML）。该框架采用无先验知识的判别性位置发现策略自动筛选病变区域，避免了传统解剖标志检测方法的专家依赖性和病灶漏检；通过在图像块层级融合多模态特征捕获多维度疾病表征，并联合学习局部图像块以保留空间结构信息，解决了图像块展平导致的信息损失问题。基于ADNI数据库842例受试者的实验验证表明，该方法在关键区域定位和疾病诊断方面均显著优于现有方法，为早期轻度认知障碍（MCI）的计算机辅助诊断提供了更优解决方案。</p> -->
                            <p>This study addresses the limitations of single-modality methods and spatial information loss in patch-based approaches for Alzheimer's disease (AD) diagnosis by proposing a Patch-based Deep Multi-Modal Learning (PDMML) framework. The framework features a prior-free discriminative location discovery strategy to automatically identify potential lesion areas, eliminating reliance on anatomical landmarks and expert experience. It innovatively integrates multimodal features at the patch level to capture comprehensive disease representations while preserving spatial information through joint patch learning, overcoming the flattening-induced information loss. Evaluated on 842 ADNI subjects, PDMML demonstrates superior performance in both discriminative region localization and brain disease diagnosis, offering a robust computer-aided solution for early mild cognitive impairment (MCI) detection.</p>
                        </div>
                    </div>
                </div>
            </div>
			<div class="row">
				<div class="col-md-12">
					<!-- <ol class="papers-list" id="grants5">
						<li>
							<b>Fangyu Liu</b>, Hao Wang, Weilin Zang, Ye Li, Fangmin Sun*. <u> Influencing Factors Mining and Modeling of Energy Expenditure in Running Based on Wearable Sensors</u>. <i><b> International Conference on Sports Technology and Performance Analysis</b>, 2024</i>. (ICSTPA 2024, <i><b>Accept</b></i>)[<a href="https://fangyuliu-2023.github.io/index_english.html#publications">Link</a>][<a href="team/activities/Poster2.pdf" target="_blank">Poster</a>]
						</li>
						<li>
							Ruijie Shao, Hao Wang, <b>Fangyu Liu</b>, Fangmin Sun*. <u> Gait Recognition Method Based on Wearable Sensor Information Fusion</u>. <i><b> The 7th International Conference on Biological Information and Biomedical Engineering</b>, 2024</i>. (BIBE 2024)[<a href="https://ieeexplore.ieee.org/abstract/document/10830681" target="_blank">Link</a>][<a href="team/activities/Poster1.pdf" target="_blank">Poster</a>][<a href="team/activities/BIBE 2024 - Most Influencial Paper Award.pdf" target="_blank">Award</a>][<a href="https://mp.weixin.qq.com/s/JQhbjR142JafRyYETr95fg" target="_blank">Promotion</a>]
						</li>
						<li>
							Jingchao Wang, Weimin Li*, Alex Munyole Luvembe, Xiao Yu, Xinyi Zhang, <b>Fangyu Liu</b>, Fangfang Liu, Hao Wang, Zhenhai Wang, Qun Jin. <u> Exploring multi-granularity contextual semantics for fully inductive knowledge graph completion</u>. <i><b> Expert Systems With Applications</b>, 2025</i>. (CCF-C, Q1 of CAS, Q1 of JCR, IF:7.5)[<a href="https://doi.org/10.1016/j.eswa.2024.125407" target="_blank">Link</a>]
						</li>
						<li>
							Xiao Liu, Weimin Li*, Shang Miao, <b>Fangyu Liu</b>, Ke Han, Tsigabu Teame Bezabih. <u> HAMMF: Hierarchical attention-based multi-task and multi-modal fusion model for computer-aided diagnosis of Alzheimer's disease</u>. <i><b> Computers in Biology and Medicine</b>, 2024</i>. (Q2 of CAS, Q1 of JCR, IF:7)[<a href="https://doi.org/10.1016/j.compbiomed.2024.108564" target="_blank">Link</a>]
						</li>
						<li>
							<b>Fangyu Liu</b>, Shizhong Yuan, Weimin Li*, Qun Xu, Xing Wu, Ke Han*, Jingchao Wang, Shang Miao. <u> Multi-task joint learning network based on adaptive patch pruning for Alzheimer's disease diagnosis and clinical score prediction</u>. <i><b> Biomedical Signal Processing and Control</b>, 2024</i>. (Q2 of CAS, Q1 of JCR, IF:4.9)[<a href="https://doi.org/10.1016/j.bspc.2024.106398" target="_blank">Link</a>]
						</li>
						<li>
							Jingchao Wang, Weimin Li*, Fangfang Liu, Zhenhai Wang, Alex Munyole Luvembe, Qun Jin, Quanke Pan, <b>Fangyu Liu</b>. <u> ConeE: Global and local context-enhanced embedding for inductive knowledge graph completion</u>. <i><b> Expert Systems With Applications</b>, 2024</i>. (CCF-C, Q1 of CAS, Q1 of JCR, IF:7.5)[<a href="https://doi.org/10.1016/j.eswa.2023.123116" target="_blank">Link</a>]
						</li>
						<li>
							Hongli Cheng, Shizhong Yuan, Weimin Li*, Xiao Yu, <b>Fangyu Liu</b>, Xiao Liu, Tsigabu Teame Bezabih. <u> De-accumulated error collaborative learning framework for predicting Alzheimer's disease progression</u>. <i><b> Biomedical Signal Processing and Control</b>, 2024</i>. (Q2 of CAS, Q1 of JCR, IF:4.9)[<a href="https://doi.org/10.1016/j.bspc.2023.105767" target="_blank">Link</a>]
						</li>
						<li>
							Shang Miao, Qun Xu, Weimin Li*, Chao Yang*, Bin Sheng, <b>Fangyu Liu</b>, Tsigabu T. Bezabih, Xiao Yu. <u> MMTFN: Multi-modal Multi-scale transformer fusion network for Alzheimer's disease diagnosis</u>. <i><b> International Journal of Imaging Systems and Technology</b>, 2024</i>. (Q4 of CAS, Q2 of JCR, IF:3)[<a href="https://doi.org/10.1002/ima.22970" target="_blank">Link</a>]
						</li>
						<li>
							<b>Fangyu Liu</b>, Shizhong Yuan, Weimin Li*, Qun Xu**, Bin Sheng. <u> Patch-based deep multi-modal learning framework for Alzheimer's disease diagnosis using multi-view neuroimaging</u>. <i><b> Biomedical Signal Processing and Control</b>, 2023</i>. (Q2 of CAS, Q1 of JCR, IF:4.9)[<a href="https://doi.org/10.1016/j.bspc.2022.104400" target="_blank">Link</a>]
						</li>
						<li>
							Jingchao Wang, Weimin Li*, Fangfang Liu, Zhenhai Wang, Alex Munyole Luvembe, Qun Jin, Quanke Pan, <b>Fangyu Liu</b>. <u> ConeE: Global and Local Context-enhanced Embedding for Inductive Knowledge Graph Completion</u>. <i><b> Expert Systems With Applications</b>, 2023</i>. (Q1 of CAS, Q1 of JCR, IF:8.5, <i><b>Accept</b></i>)[<a href="https://fangyuliu-2023.github.io/index_english.html#publications">Link</a>]
						</li>
					</ol> -->
					<ol class="jump-list" id="grants6">
                        <li>
                            <b><a href="paper_english.html">More Details...</a></b>
                        </li>
                    </ol>
				</div>
			</div>

			<h2 id="activities" class="page-header">Academic Service</h2>
			<div class="row">
				<div class="col-md-12">
<!--					<ul class="activities-list" id="grants7">-->
<!--						<li>XXX Artificial Intelligence Conference</li>-->
<!--					</ul>-->

<!--					<h4><b>Editorial Board</b></h4>-->
<!--					<ul class="editor-list" id="grants8">-->
<!--						<li>The Guest Editor of the magazine XXX</li>-->
<!--						<li>The Guest Editor of the magazine XXX</li>-->
<!--						<li>The Guest Editor of the magazine XXX</li>-->
<!--					</ul>-->
                    <h4><b>Reviewer</b></h4>
					<ul class="reviewer-list" id="grants9">
						<li>Reviewer for the journal "IEEE Journal of Biomedical and Health Informatics"</li>
						<li>Reviewer for the journal "Knowledge-Based Systems"</li>
						<li>Reviewer for the journal "Scientific Reports"</li>
						<li>Reviewer for the 21st IEEE International Conference on Ubiquitous Intelligence and Computing (UIC 2024)</li>
					</ul>
					<h4><b>Volunteer</b></h4>
					<ul class="chair-list" id="grants10">
						<li>Volunteer for the 15th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM BCB 2024)</li>
						<li>Volunteer for the national science-popularizing public activity&#45;&#45;"the 20th Public Science Day held by the Chinese Academy of Sciences"</li>
					</ul>
<!--					<h4><b>Program Chair</b></h4>-->
<!--					<ul class="chair-list" id="grants10">-->
<!--						<li>The program chairman of the XXX</li>-->
<!--						<li>The program chair member of the XXX</li>-->
<!--						<li>The Workshop/Special session Chair of the XXX</li>-->
<!--					</ul>-->
<!--					<h4><b>Organizer</b></h4>-->
<!--					<ul class="organizer-list" id="grants11">-->
<!--						<li>The organizer of the Special Session XXX</li>-->
<!--					</ul>-->

				</div>
			</div>

			<h2 id="award" class="page-header">Honors and Awards</h2>
			<div class="row">
				<div class="col-md-12">
					<ul class="award-list" id="grants17">
<!--						<li>2022-2023, Shanghai University Second Class Academic Scholarship</li>-->
<!--						<li>2021-2022, Shanghai University Second Class Academic Scholarship</li>-->
<!--						<li>2021-2022, "Excellent Party Branch" of Shanghai University School of Computer Engineering and Science</li>-->
<!--						<li>2021-2022, Shanghai University Graduate Student Party Branch "Top 10 Theme Party Day Activities"</li>-->
<!--						<li>2021, Shanghai University Graduate Student Summer Social Practice School-level Outstanding Programs</li>-->
<!--						<li>2020-2021, Shanghai University Second Class Academic Scholarship</li>-->
<!--						<li>2020-2021, Shanghai University Graduate Student Youth League Branch Student Advanced Collective</li>-->
<!--						<li>2020-2021, Student Community "Activists"</li>-->
						<li>2020, Outstanding Graduates of Hunan Province, China</li>
						<li>2020, Outstanding Graduates of the University of South China</li>
<!--						<li>2018-2019, 1-Grade Scholarship of the University of South China</li>-->
<!--						<li>2018-2019, Excellent Cadre of Students of the University of South China</li>-->
<!--						<li>2019, Third Prize of the 12th China University Student Computer Design Competition in Central and South China Region</li>-->
<!--						<li>2019, Second Prize of the 16th May Day Mathematical Modeling Competition</li>-->
<!--						<li>2019, The 10th Blue Bridge Cup Hunan Province C/C++ Programming University B Group Second Prize</li>-->
<!--						<li>2018, First Prize of the First National Green Computing Competition for Colleges and Universities (Open Source Labeling Category)</li>-->
<!--						<li>2017-2018, National Inspirational Scholarship</li>-->
<!--						<li>2017-2018, Excellent League Cadres of the University of South China</li>-->
<!--						<li>2017-2018, Excellent All-round Student of the University of South China</li>-->
<!--						<li>2016-2017, 1-Grade Scholarship of the University of South China</li>-->
<!--						<li>2016-2017, Excellent Cadre of Students of the University of South China</li>-->
<!--						<li>2016-2017, Excellent All-round Student of the University of South China</li>-->
<!--						<li>2017, First Prize of the 13th ACM Programming Contest at the University of South China</li>-->
<!--						<li>2016, First Prize of "Learning the Law, Complying with the Law, and Using the Law" Skit Competition at the University of South China</li>-->
<!--						<li>2015, "Excellent All-round Student" in high school</li>-->
					</ul>
				</div>
			</div>
		</div>
	</div>
</div>


<footer class="footer">
	<div class="container">
		<p class="text-muted">Homepage Create:09/2023, Last Modified:04/2025, Copyright © Fangyu Liu 2023-2025</p>
	</div>
	<div class="color_wrapper"></div>
</footer>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
	<script src="./Javascript/jquery.min.js"></script>
    <script src="./Javascript/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 10 bug -->
<!--    <script src="js/ie10-viewport-bug-workaround.js"></script>-->
	<!-- 为按钮添加点击事件 -->
	<script>
		$(document).ready(function(){
		  // 监听窗口宽度变化
		  $(window).resize(function() {
			if ($(window).width() < 768) { // 假设小屏的阈值为768px
			  // 先移除之前的事件监听器，预防尺寸变化重复绑定监听事件
			  $(".navbar-toggle").off('click');
			  $('.navbar-collapse a').off('click');

			  // 监听导航栏按钮的点击事件
			  $(".navbar-toggle").click(function(){
				$(".bs-navbar-collapse").toggleClass("in");
				$(this).toggleClass("collapsed"); // 切换按钮的collapsed类
			  });

			  // 监听导航栏链接的点击事件
			  $('.navbar-collapse a').on('click', function () {
				  $(".navbar-toggle").click(); // 触发导航栏按钮的点击事件，使导航栏收缩
			  });
			}
		  }).trigger('resize'); // 初始触发一次resize事件以应用事件监听器
		});
	</script>
  </body>

</html>